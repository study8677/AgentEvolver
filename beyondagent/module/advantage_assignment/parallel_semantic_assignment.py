import torch
import verl.utils.torch_functional as verl_F
from openai import AsyncOpenAI
import os
from loguru import logger
import time
import traceback
from tqdm import tqdm
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict, Optional, Literal
import threading
from dataclasses import dataclass

__all__ = [
    "evaluate_step_flags_parallel",    # å¹¶è¡Œç‰ˆæœ¬çš„stepè¯„ä¼°
    "apply_step_mask_vectorized",      # å‘é‡åŒ–çš„maskåº”ç”¨
    "ParallelSemanticProcessor",       # ç»Ÿä¸€çš„å¤„ç†å™¨ç±»
]

@dataclass
class EvaluationTask:
    """è¯„ä¼°ä»»åŠ¡çš„æ•°æ®ç»“æ„"""
    sample_idx: int
    step_idx: int
    query: str
    rollout: str
    step_text: str
    overall_adv: float

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœçš„æ•°æ®ç»“æ„"""
    sample_idx: int
    step_idx: int
    is_good: bool
    response_time: float

# å…¨å±€å˜é‡å­˜å‚¨vLLMæ¨¡å‹å’Œtokenizerï¼ˆç”¨äºæœ¬åœ°è¯„ä¼°ï¼‰
_vllm_model = None
_vllm_tokenizer = None
_model_lock = threading.Lock()

def _get_overall_advantage(advantages_tensor, mask=None):
    """
    ä»advantages tensorä¸­è·å–overall advantageå€¼
    åœ¨GRPOä¸­ï¼Œæ‰€æœ‰æœ‰æ•ˆtokenå…±äº«ä¸€ä¸ªadvantageï¼Œæˆ‘ä»¬éœ€è¦æ­£ç¡®æå–è¿™ä¸ªå€¼
    
    Args:
        advantages_tensor: advantage tensor, shape (resp_len,) 
        mask: æ ‡è¯†éœ€è¦è®­ç»ƒçš„tokenä½ç½®çš„maskï¼Œshape (resp_len,)
              å¯ä»¥æ˜¯loss_maskæˆ–response_maskï¼Œå–å†³äºå¤–éƒ¨ä¼ å…¥
    
    Returns:
        float: æå–åˆ°çš„overall advantageå€¼
    """
    if advantages_tensor.dim() == 0:  # scalar
        return advantages_tensor.item()
    
    if advantages_tensor.dim() == 1:  # shape: (resp_len,)
        # ä¼˜å…ˆä½¿ç”¨maskæ¥æå–æœ‰æ•ˆadvantage
        if mask is not None:
            valid_advantages = advantages_tensor[mask.bool()]
            if len(valid_advantages) > 0:
                # åœ¨GRPOä¸­ï¼Œæ‰€æœ‰æœ‰æ•ˆtokençš„advantageåº”è¯¥ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ªå³å¯
                return valid_advantages[0].item()
            else:
                # maskä¸­æ²¡æœ‰æœ‰æ•ˆtokenï¼Œè¿”å›0
                return 0.0
        else:
            # fallback: æ²¡æœ‰maskæ—¶ï¼Œå¯»æ‰¾ç¬¬ä¸€ä¸ªéé›¶å€¼
            non_zero_mask = torch.abs(advantages_tensor) > 1e-8
            if non_zero_mask.any():
                return advantages_tensor[non_zero_mask][0].item()
            else:
                return 0.0
    
    # å…¶ä»–ç»´åº¦ä¸æ”¯æŒ
    raise ValueError(f"Unsupported advantages_tensor shape: {advantages_tensor.shape}")

def _build_prompt(query: str, rollout: str, step: str, overall_adv: float) -> list[dict]:
    """
    æ„é€ å¯¹è¯æ¶ˆæ¯ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰
    
    Args:
        overall_adv: çœŸæ­£çš„å…±äº«advantageå€¼ï¼ˆGRPOä¸­æ‰€æœ‰tokenå…±äº«ï¼‰ï¼Œ
                    ä¸æ˜¯sum()åè¢«åºåˆ—é•¿åº¦æ”¾å¤§çš„é”™è¯¯å€¼
    """
    polarity = "positive" if overall_adv > 0 else "negative"
    sys = "You are an expert reward-model evaluator. Reply with **exactly one word**, either **GOOD** or **BAD** â€“ no explanations."
    user = (
        f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
        f"USER QUERY\n{query}\n\n"
        f"ASSISTANT FULL ANSWER\n{rollout}\n\n"
        f"CURRENT ASSISTANT STEP\n{step}\n"
        f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n"
        f"The total advantage (quality score) of the full answer is "
        f"**{overall_adv:+.4f}** â†’ this is {polarity} "
        f"(positive if > 0, negative if < 0).\n\n"
        f"**Task**\n"
        f"Does the *current assistant step* improve (GOOD) or harm (BAD) "
        f"the final answer given the user query and the overall advantage?"
    )
    return [{"role": "system", "content": sys}, {"role": "user", "content": user}]

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# æœ¬åœ°æ¨¡å‹è¯„ä¼°ï¼ˆvLLMï¼‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def _initialize_vllm_model(model_name: str = "Qwen/Qwen2.5-7B-Instruct"):
    """åˆå§‹åŒ–vLLMæ¨¡å‹å’Œtokenizerï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global _vllm_model, _vllm_tokenizer
    
    with _model_lock:
        if _vllm_model is None:
            print(f"[vLLM] Initializing model: {model_name}")
            try:
                # æ–°å¢ï¼švLLMç›¸å…³å¯¼å…¥
                from transformers import AutoTokenizer
                from vllm import LLM, SamplingParams
                
                # åˆå§‹åŒ–tokenizer
                _vllm_tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                # åˆå§‹åŒ–vLLMå¼•æ“
                _vllm_model = LLM(
                    model=model_name,
                    gpu_memory_utilization=0.8,  # è°ƒæ•´GPUå†…å­˜ä½¿ç”¨ç‡
                    tensor_parallel_size=1,      # å•GPU
                    dtype="auto",                # è‡ªåŠ¨é€‰æ‹©æ•°æ®ç±»å‹
                    trust_remote_code=True
                )
                print(f"[vLLM] Model initialized successfully")
            except Exception as e:
                print(f"[vLLM] Failed to initialize model: {e}")
                raise e
    
    return _vllm_model, _vllm_tokenizer

async def _vllm_safe_query(model, 
                          tokenizer,
                          messages: list[dict], 
                          semaphore: asyncio.Semaphore,
                          max_retries: int = 3) -> str:
    """ä½¿ç”¨vLLMè¿›è¡Œå®‰å…¨çš„æœ¬åœ°æ¨ç†"""
    async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºæ–‡æœ¬
                text = tokenizer.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True,
                    enable_thinking=False  # å¯¹äºç®€å•çš„GOOD/BADåˆ¤æ–­ï¼Œä¸éœ€è¦thinkingæ¨¡å¼
                )
                
                # é…ç½®é‡‡æ ·å‚æ•°ï¼ˆä¸ºäº†è·å¾—ç¡®å®šæ€§ç»“æœï¼‰
                from vllm import SamplingParams
                sampling_params = SamplingParams(
                    temperature=0.0,     # ç¡®å®šæ€§è¾“å‡º
                    top_p=1.0,
                    top_k=-1,
                    max_tokens=10,       # åªéœ€è¦ä¸€ä¸ªè¯
                    stop=None
                )
                
                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„vLLMæ¨ç†
                def run_vllm():
                    outputs = model.generate([text], sampling_params)
                    return outputs[0].outputs[0].text.strip()
                
                # ä½¿ç”¨asyncio.to_threadå°†åŒæ­¥è°ƒç”¨è½¬ä¸ºå¼‚æ­¥
                result = await asyncio.to_thread(run_vllm)
                
                return result
                
            except Exception as e:
                last_exception = e
                if attempt < max_retries - 1:
                    await asyncio.sleep(1.0 * (attempt + 1))
        
        raise last_exception

async def _evaluate_single_task_vllm(model,
                                    tokenizer,
                                    task: EvaluationTask,
                                    semaphore: asyncio.Semaphore) -> EvaluationResult:
    """ä½¿ç”¨vLLMè¯„ä¼°å•ä¸ªä»»åŠ¡"""
    start_time = time.time()
    
    try:
        messages = _build_prompt(task.query, task.rollout, task.step_text, task.overall_adv)
        answer = await _vllm_safe_query(model, tokenizer, messages, semaphore)
        
        answer_upper = answer.upper()
        is_good = answer_upper.startswith("G") or "GOOD" in answer_upper
        
        response_time = time.time() - start_time
        
        return EvaluationResult(
            sample_idx=task.sample_idx,
            step_idx=task.step_idx,
            is_good=is_good,
            response_time=response_time
        )
        
    except Exception as e:
        response_time = time.time() - start_time
        print(f"[parallel_eval] Failed to evaluate sample {task.sample_idx}, step {task.step_idx}: {e}")
        
        # å¤±è´¥æ—¶ä½¿ç”¨éšæœºfallback
        import random
        is_good = random.choice([True, False])
        
        return EvaluationResult(
            sample_idx=task.sample_idx,
            step_idx=task.step_idx,
            is_good=is_good,
            response_time=response_time
        )

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# APIè¯„ä¼°ï¼ˆOpenAIå…¼å®¹ï¼‰- å¢å¼ºçš„é‡è¯•æœºåˆ¶
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

async def _async_safe_query(client: AsyncOpenAI, 
                           model: str, 
                           messages: list[dict], 
                           semaphore: asyncio.Semaphore,
                           max_retries: int = 200) -> str:
    """
    å¼‚æ­¥å®‰å…¨çš„APIè°ƒç”¨ï¼Œå¢å¼ºçš„é‡è¯•æœºåˆ¶ï¼Œä¸“é—¨å¤„ç†429é”™è¯¯
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        model: æ¨¡å‹åç§°
        messages: æ¶ˆæ¯åˆ—è¡¨
        semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤200æ¬¡
    
    Returns:
        APIå“åº”å†…å®¹
    """
    async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                response = await client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=0.0,
                    timeout=30,
                    max_tokens=10,
                )
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                last_exception = e
                error_str = str(e).lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯429é”™è¯¯
                is_rate_limit_error = (
                    "429" in error_str or 
                    "rate limit" in error_str or
                    "limit_requests" in error_str or
                    "exceeded your current requests" in error_str
                )
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–å¯é‡è¯•çš„é”™è¯¯
                is_retryable_error = (
                    "timeout" in error_str or
                    "connection" in error_str or
                    "500" in error_str or
                    "502" in error_str or
                    "503" in error_str or
                    "504" in error_str
                )
                
                if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    if is_rate_limit_error:
                        # 429é”™è¯¯ï¼šä½¿ç”¨æŒ‡æ•°é€€é¿ï¼Œä½†æœ‰ä¸Šé™
                        # åŸºç¡€ç­‰å¾…æ—¶é—´ï¼š1ç§’ï¼Œæ¯æ¬¡ç¿»å€ï¼Œæœ€å¤§60ç§’
                        base_wait = min(1.0 * (2 ** min(attempt, 6)), 60.0)
                        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼Œé¿å…æ‰€æœ‰è¯·æ±‚åŒæ—¶é‡è¯•
                        import random
                        jitter = random.uniform(0.1, 0.3) * base_wait
                        wait_time = base_wait + jitter
                        
                        print(f"[API Retry] 429 Rate limit hit, attempt {attempt + 1}/{max_retries}, waiting {wait_time:.2f}s")
                        await asyncio.sleep(wait_time)
                        
                    elif is_retryable_error:
                        # å…¶ä»–å¯é‡è¯•é”™è¯¯ï¼šè¾ƒçŸ­çš„ç­‰å¾…æ—¶é—´
                        wait_time = min(2.0 * (attempt + 1), 10.0)
                        print(f"[API Retry] Retryable error, attempt {attempt + 1}/{max_retries}, waiting {wait_time:.2f}s: {e}")
                        await asyncio.sleep(wait_time)
                        
                    else:
                        # ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç«‹å³å¤±è´¥
                        print(f"[API Error] Non-retryable error, failing immediately: {e}")
                        break
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    if is_rate_limit_error:
                        print(f"[API Error] Rate limit exceeded after {max_retries} attempts")
                    else:
                        print(f"[API Error] Max retries ({max_retries}) exceeded: {e}")
        
        raise last_exception

async def _evaluate_single_task_api(client: AsyncOpenAI,
                                  model_name: str,
                                  task: EvaluationTask,
                                  semaphore: asyncio.Semaphore,
                                  max_retries: int = 200) -> EvaluationResult:
    """
    ä½¿ç”¨APIè¯„ä¼°å•ä¸ªä»»åŠ¡ï¼Œå¢å¼ºé‡è¯•æœºåˆ¶
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        model_name: æ¨¡å‹åç§°
        task: è¯„ä¼°ä»»åŠ¡
        semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    """
    start_time = time.time()
    
    try:
        messages = _build_prompt(task.query, task.rollout, task.step_text, task.overall_adv)
        answer = await _async_safe_query(client, model_name, messages, semaphore, max_retries)
        
        answer_upper = answer.upper()
        is_good = answer_upper.startswith("G") or "GOOD" in answer_upper
        
        response_time = time.time() - start_time
        
        return EvaluationResult(
            sample_idx=task.sample_idx,
            step_idx=task.step_idx,
            is_good=is_good,
            response_time=response_time
        )
        
    except Exception as e:
        response_time = time.time() - start_time
        print(f"[parallel_eval] Failed to evaluate sample {task.sample_idx}, step {task.step_idx} after all retries: {e}")
        
        # å¤±è´¥æ—¶ä½¿ç”¨éšæœºfallback
        import random
        is_good = random.choice([True, False])
        
        return EvaluationResult(
            sample_idx=task.sample_idx,
            step_idx=task.step_idx,
            is_good=is_good,
            response_time=response_time
        )

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ç»Ÿä¸€çš„å¹¶è¡Œè¯„ä¼°æ¥å£
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

async def evaluate_step_flags_parallel(tokenizer,
                                     batch,
                                     model_name: str = "qwen-max",
                                     evaluation_type: Literal["local", "api"] = "api",
                                     max_concurrent: int = 20,
                                     batch_size_limit: int = 100,
                                     mask_tensor: torch.Tensor = None,
                                     api_max_retries: int = 200) -> Tuple[List[List[bool]], Dict]:
    """
    å¹¶è¡Œè¯„ä¼°step flagsï¼Œæ”¯æŒæœ¬åœ°æ¨¡å‹å’ŒAPIä¸¤ç§æ–¹å¼
    å¯¹äºadvantage=0çš„æ ·æœ¬è·³è¿‡è¯„ä¼°ï¼Œç›´æ¥è¿”å›GOOD
    
    Args:
        tokenizer: åˆ†è¯å™¨
        batch: æ•°æ®æ‰¹æ¬¡
        model_name: æ¨¡å‹åç§°
        evaluation_type: è¯„ä¼°ç±»å‹ï¼Œ"local"ä½¿ç”¨vLLMæœ¬åœ°æ¨¡å‹ï¼Œ"api"ä½¿ç”¨APIè°ƒç”¨
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        batch_size_limit: å•æ‰¹æ¬¡å¤„ç†çš„æœ€å¤§ä»»åŠ¡æ•°
        mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensorï¼Œshape (bs, resp_len)
                    å¯ä»¥æ˜¯loss_maskæˆ–response_maskï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„loss_mask
        api_max_retries: APIè°ƒç”¨çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç‰¹åˆ«ç”¨äºå¤„ç†429é”™è¯¯
        
    Returns:
        (flags_per_sample, stats): è¯„ä¼°ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
    """
    batch_size = len(batch.batch['prompts'])
    print(f"[parallel_eval] Starting parallel evaluation for {batch_size} samples using {evaluation_type} mode")
    print(f"[parallel_eval] Model: {model_name}, API max retries: {api_max_retries}")
    
    # æ£€æŸ¥å¿…è¦çš„è¾“å…¥
    if 'steps' not in batch.non_tensor_batch:
        raise ValueError("batch.non_tensor_batch['steps'] is required but not found")
    
    # æ ¹æ®è¯„ä¼°ç±»å‹åˆå§‹åŒ–
    if evaluation_type == "local":
        # åˆå§‹åŒ–vLLMæ¨¡å‹
        try:
            vllm_model, vllm_tokenizer = _initialize_vllm_model(model_name)
            api_client = None
        except Exception as e:
            print(f"[parallel_eval] Failed to initialize vLLM model, using random fallback: {e}")
            return _apply_fallback_strategy_parallel(batch), {"fallback_used": True, "error": str(e), "evaluation_type": evaluation_type}
    elif evaluation_type == "api":
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            print("[parallel_eval] No API key found, using random fallback")
            return _apply_fallback_strategy_parallel(batch), {"fallback_used": True, "evaluation_type": evaluation_type}
        
        api_client = AsyncOpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        vllm_model = vllm_tokenizer = None
    else:
        raise ValueError(f"Unsupported evaluation_type: {evaluation_type}. Must be 'local' or 'api'")
    
    # å‡†å¤‡æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ï¼Œè·³è¿‡advantage=0çš„æ ·æœ¬
    all_tasks = []
    flags_per_sample = [[] for _ in range(batch_size)]
    skipped_samples = 0
    
    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„mask_tensorï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥åˆ™ä½¿ç”¨é»˜è®¤çš„loss_mask
    if mask_tensor is not None:
        response_mask = mask_tensor
        print(f"[parallel_eval] Using external mask tensor with shape {mask_tensor.shape}")
        
        # éªŒè¯mask tensorçš„å½¢çŠ¶
        response_length = batch.batch["responses"].size(1)
        if response_mask.shape != (batch_size, response_length):
            raise ValueError(f"mask_tensor shape {response_mask.shape} doesn't match expected shape ({batch_size}, {response_length})")
    else:
        # ä½¿ç”¨é»˜è®¤çš„loss_mask
        response_length = batch.batch["responses"].size(1)
        response_mask = batch.batch["loss_mask"][:, -response_length:]
        print(f"[parallel_eval] Using default loss_mask")

    for sample_idx in range(batch_size):
        query = tokenizer.decode(batch.batch["prompts"][sample_idx], skip_special_tokens=True)
        rollout = tokenizer.decode(batch.batch["responses"][sample_idx], skip_special_tokens=True)
        steps = batch.non_tensor_batch["steps"][sample_idx]
        
        # ä½¿ç”¨ä¼ å…¥çš„maskæå–æ­£ç¡®çš„overall advantage
        sample_mask = response_mask[sample_idx]
        
        overall_adv = _get_overall_advantage(
            batch.batch["advantages"][sample_idx], 
            sample_mask
        )
        
        # æ–°å¢ï¼šå¦‚æœadvantageä¸º0ï¼Œç›´æ¥è®¾ç½®æ‰€æœ‰stepä¸ºGOODï¼Œè·³è¿‡APIè°ƒç”¨
        if abs(overall_adv) < 1e-8:  # ä½¿ç”¨å°çš„é˜ˆå€¼å¤„ç†æµ®ç‚¹ç²¾åº¦é—®é¢˜
            print(f"[parallel_eval] Sample {sample_idx}: advantageâ‰ˆ0 ({overall_adv:.6f}), skipping evaluation, returning all GOOD")
            flags_per_sample[sample_idx] = [True] * len(steps)  # æ‰€æœ‰stepéƒ½æ ‡è®°ä¸ºGOOD
            skipped_samples += 1
            continue
        
        # ä¸ºéé›¶advantageçš„æ ·æœ¬åˆ›å»ºè¯„ä¼°ä»»åŠ¡
        for step_idx, step_text in enumerate(steps):
            task = EvaluationTask(
                sample_idx=sample_idx,
                step_idx=step_idx,
                query=query,
                rollout=rollout,
                step_text=step_text,
                overall_adv=overall_adv
            )
            all_tasks.append(task)
    
    total_tasks = len(all_tasks)
    print(f"[parallel_eval] Total tasks to process: {total_tasks}")
    print(f"[parallel_eval] Skipped {skipped_samples} samples with advantage=0")
    
    if total_tasks == 0:
        # æ‰€æœ‰æ ·æœ¬éƒ½è¢«è·³è¿‡äº†
        print("[parallel_eval] No tasks to process, all samples had advantage=0")
        if api_client:
            await api_client.close()
        return flags_per_sample, {
            "total_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_api_time": 0,
            "avg_api_time": 0,
            "max_concurrent": max_concurrent,
            "fallback_used": False,
            "skipped_samples": skipped_samples,
            "evaluation_type": evaluation_type,
            "api_max_retries": api_max_retries
        }
    
    # åˆ†æ‰¹å¤„ç†ä»»åŠ¡ï¼ˆé¿å…å†…å­˜è¿‡å¤§ï¼‰
    all_results = []
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # ä½¿ç”¨è¿›åº¦æ¡
    with tqdm(total=total_tasks, desc=f"[parallel_eval] Processing tasks ({evaluation_type})") as pbar:
        for i in range(0, total_tasks, batch_size_limit):
            batch_tasks = all_tasks[i:i + batch_size_limit]
            
            # æ ¹æ®è¯„ä¼°ç±»å‹åˆ›å»ºåç¨‹ä»»åŠ¡
            if evaluation_type == "local":
                coroutines = [
                    _evaluate_single_task_vllm(vllm_model, vllm_tokenizer, task, semaphore)
                    for task in batch_tasks
                ]
            else:  # api
                coroutines = [
                    _evaluate_single_task_api(api_client, model_name, task, semaphore, api_max_retries)
                    for task in batch_tasks
                ]
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*coroutines, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in batch_results:
                if isinstance(result, Exception):
                    print(f"[parallel_eval] Task failed with exception: {result}")
                    continue
                all_results.append(result)
            
            pbar.update(len(batch_tasks))
    
    # æ•´ç†ç»“æœåˆ°å·²ç»åˆå§‹åŒ–çš„flags_per_sampleä¸­
    # æŒ‰sample_idxå’Œstep_idxæ’åº
    all_results.sort(key=lambda x: (x.sample_idx, x.step_idx))
    
    for result in all_results:
        # ä¸ºéè·³è¿‡çš„æ ·æœ¬å¡«å……ç»“æœ
        if not flags_per_sample[result.sample_idx]:  # å¦‚æœè¿˜æ˜¯ç©ºåˆ—è¡¨
            flags_per_sample[result.sample_idx] = []
        flags_per_sample[result.sample_idx].append(result.is_good)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_time = sum(r.response_time for r in all_results)
    avg_time = total_time / len(all_results) if all_results else 0
    
    stats = {
        "total_tasks": total_tasks,
        "successful_tasks": len(all_results),
        "failed_tasks": total_tasks - len(all_results),
        "total_api_time": total_time,
        "avg_api_time": avg_time,
        "max_concurrent": max_concurrent,
        "fallback_used": False,
        "skipped_samples": skipped_samples,
        "evaluation_type": evaluation_type,
        "model_name": model_name,
        "api_max_retries": api_max_retries
    }
    
    print(f"[parallel_eval] Completed. Stats: {stats}")
    
    # æ¸…ç†èµ„æº
    if api_client:
        await api_client.close()
    
    return flags_per_sample, stats

def _apply_fallback_strategy_parallel(batch) -> List[List[bool]]:
    """å¹¶è¡Œfallbackç­–ç•¥"""
    import random
    
    flags_per_sample = []
    for steps in batch.non_tensor_batch["steps"]:
        flags = [random.choice([True, False]) for _ in steps]
        flags_per_sample.append(flags)
    
    return flags_per_sample

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# å‘é‡åŒ–çš„maskåº”ç”¨ï¼ˆä¿æŒä¸å˜ï¼‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def apply_step_mask_vectorized(batch,
                             step_flags: List[List[bool]],
                             good_scale: float = 1.0,
                             bad_scale: float = 0.2,
                             neg_bad_scale: float = -0.2,
                             mask_tensor: torch.Tensor = None) -> Dict:
    """
    å‘é‡åŒ–ç‰ˆæœ¬çš„step maskåº”ç”¨ï¼Œé¿å…åµŒå¥—å¾ªç¯
    å¯¹äºadvantage=0çš„æ ·æœ¬è·³è¿‡å¤„ç†
    
    Args:
        batch: æ‰¹æ¬¡æ•°æ®
        step_flags: stepè¯„ä¼°ç»“æœ
        good_scale, bad_scale, neg_bad_scale: ç¼©æ”¾å› å­
        mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensorï¼Œshape (bs, resp_len)
                    å¯ä»¥æ˜¯loss_maskæˆ–response_maskï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„loss_mask
    
    Returns:
        stats: åº”ç”¨ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"[vectorized_mask] Starting vectorized mask application")
    
    # æ£€æŸ¥å¿…è¦çš„è¾“å…¥
    if 'step_ids' not in batch.batch:
        raise ValueError("batch.batch['step_ids'] is required but not found")
    
    adv = batch.batch["advantages"]  # (bs, resp_len)
    step_ids = batch.batch["step_ids"].to(adv.device)  # (bs, resp_len)
    
    bs, resp_len = adv.shape
    
    if len(step_flags) != bs:
        raise ValueError(f"step_flags length ({len(step_flags)}) != batch size ({bs})")
    
    # åˆå§‹åŒ–scaleä¸ºå…¨1
    scale = torch.ones_like(adv)
    
    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„mask_tensorè®¡ç®—overall advantage
    overall_advs = []
    
    # ä½¿ç”¨ä¼ å…¥çš„mask_tensorï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥åˆ™ä½¿ç”¨é»˜è®¤çš„loss_mask
    if mask_tensor is not None:
        response_mask = mask_tensor
        print(f"[vectorized_mask] Using external mask tensor with shape {mask_tensor.shape}")
        
        # éªŒè¯mask tensorçš„å½¢çŠ¶
        if response_mask.shape != (bs, resp_len):
            raise ValueError(f"mask_tensor shape {response_mask.shape} doesn't match expected shape ({bs}, {resp_len})")
    else:
        # ä½¿ç”¨é»˜è®¤çš„loss_mask
        response_mask = batch.batch["loss_mask"][:, -resp_len:]
        print(f"[vectorized_mask] Using default loss_mask")
    
    for sample_idx in range(bs):
        sample_mask = response_mask[sample_idx]
        
        overall_adv = _get_overall_advantage(
            adv[sample_idx], 
            sample_mask
        )
        overall_advs.append(overall_adv)
    
    overall_advs = torch.tensor(overall_advs, device=adv.device)
    overall_pos = overall_advs > 0  # (bs,) bool tensor
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_samples": bs,
        "total_tokens": resp_len * bs,
        "tokens_modified": 0,
        "good_steps": 0,
        "bad_steps": 0,
        "positive_samples": overall_pos.sum().item(),
        "negative_samples": (~overall_pos).sum().item(),
        "zero_adv_samples": 0  # æ–°å¢ï¼šé›¶advantageæ ·æœ¬ç»Ÿè®¡
    }
    
    # å¤„ç†æ¯ä¸ªæ ·æœ¬ï¼ˆè¿™éƒ¨åˆ†è¿˜æ˜¯éœ€è¦å¾ªç¯ï¼Œä½†å†…éƒ¨æ˜¯å‘é‡åŒ–çš„ï¼‰
    for b in tqdm(range(bs), desc="[vectorized_mask] Processing samples"):
        current_step_flags = step_flags[b]
        overall_adv_sum = overall_advs[b].item()
        
        # æ–°å¢ï¼šå¦‚æœadvantageä¸º0ï¼Œè·³è¿‡å¤„ç†ï¼ˆä¿æŒscale=1.0ï¼‰
        if abs(overall_adv_sum) < 1e-8:
            stats["zero_adv_samples"] += 1
            continue
        
        if not current_step_flags:
            continue
            
        # è·å–å½“å‰æ ·æœ¬çš„step_idså’Œadvantages
        sample_step_ids = step_ids[b]  # (resp_len,)
        sample_adv = adv[b]  # (resp_len,)
        sample_overall_pos = overall_pos[b].item()
        
        # ä¸ºæ¯ä¸ªstepåˆ›å»ºmaskå’Œå¯¹åº”çš„scale factor
        max_step_id = len(current_step_flags)
        
        # å‘é‡åŒ–å¤„ç†ï¼šä¸ºæ¯ä¸ªstep_idåˆ›å»ºmask
        for step_id, is_good in enumerate(current_step_flags):
            # åˆ›å»ºå½“å‰stepçš„token mask
            step_mask = (sample_step_ids == step_id)  # (resp_len,)
            
            if not step_mask.any():
                continue
            
            # æ ¹æ®overall_poså’Œis_goodç¡®å®šscale factor
            if sample_overall_pos:
                factor = good_scale if is_good else bad_scale
            else:
                factor = neg_bad_scale if is_good else good_scale
            
            # åº”ç”¨scale factor
            scale[b].masked_fill_(step_mask, factor)
            
            # æ›´æ–°ç»Ÿè®¡
            tokens_in_step = step_mask.sum().item()
            stats["tokens_modified"] += tokens_in_step
            
            if is_good:
                stats["good_steps"] += 1
            else:
                stats["bad_steps"] += 1
    
    # ç¡®ä¿å¡«å……tokenï¼ˆstep_id == -1ï¼‰ä¿æŒscale=1.0
    padding_mask = (step_ids == -1)
    scale.masked_fill_(padding_mask, 1.0)
    
    # åº”ç”¨scale
    original_adv_sum = adv.sum().item()
    batch.batch["advantages"] = adv * scale
    new_adv_sum = batch.batch["advantages"].sum().item()
    
    # ä¿å­˜scaleç”¨äºè°ƒè¯•
    batch.batch["semantic_scale"] = scale
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    stats["original_adv_sum"] = original_adv_sum
    stats["new_adv_sum"] = new_adv_sum
    stats["adv_change_ratio"] = new_adv_sum / original_adv_sum if original_adv_sum != 0 else 1.0
    
    print(f"[vectorized_mask] Completed. Advantages: {original_adv_sum:.4f} -> {new_adv_sum:.4f}")
    print(f"[vectorized_mask] Modified {stats['tokens_modified']} tokens ({stats['good_steps']} good steps, {stats['bad_steps']} bad steps)")
    print(f"[vectorized_mask] Skipped {stats['zero_adv_samples']} samples with advantage=0")
    
    return stats

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# åŒæ­¥åŒ…è£…å‡½æ•°ï¼ˆæ›´æ–°ä¸ºæ”¯æŒevaluation_typeå’Œapi_max_retriesï¼‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def evaluate_step_flags(tokenizer,
                        batch,
                        good_words: tuple[str, ...] = ("GOOD",),
                        bad_words: tuple[str, ...] = ("BAD",),
                        model_name: str = "qwen-max",
                        evaluation_type: Literal["local", "api"] = "api",
                        use_parallel: bool = True,
                        max_concurrent: int = 20,
                        mask_tensor: torch.Tensor = None,
                        api_max_retries: int = 200) -> List[List[bool]]:
    """
    å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œå¯é€‰æ‹©ä½¿ç”¨å¹¶è¡Œæˆ–ä¸²è¡Œç‰ˆæœ¬ï¼Œæ”¯æŒæœ¬åœ°å’ŒAPIè¯„ä¼°
    
    Args:
        tokenizer: åˆ†è¯å™¨
        batch: æ•°æ®æ‰¹æ¬¡
        good_words, bad_words: å…¼å®¹æ€§å‚æ•°ï¼Œåœ¨å¹¶è¡Œç‰ˆæœ¬ä¸­æœªä½¿ç”¨
        model_name: æ¨¡å‹åç§°
        evaluation_type: è¯„ä¼°ç±»å‹ï¼Œ"local"ä½¿ç”¨vLLMæœ¬åœ°æ¨¡å‹ï¼Œ"api"ä½¿ç”¨APIè°ƒç”¨
        use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensor
        api_max_retries: APIè°ƒç”¨çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç‰¹åˆ«ç”¨äºå¤„ç†429é”™è¯¯
    """
    if use_parallel:
        # ä½¿ç”¨å¼‚æ­¥å¹¶è¡Œç‰ˆæœ¬
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        flags, stats = loop.run_until_complete(
            evaluate_step_flags_parallel(
                tokenizer=tokenizer,
                batch=batch,
                model_name=model_name,
                evaluation_type=evaluation_type,
                max_concurrent=max_concurrent,
                mask_tensor=mask_tensor,  # ä¼ å…¥å¤–éƒ¨mask
                api_max_retries=api_max_retries  # ä¼ å…¥APIé‡è¯•æ¬¡æ•°
            )
        )
        
        print(f"[evaluate_step_flags] Parallel execution stats: {stats}")
        return flags
    else:
        # ä½¿ç”¨åŸæ¥çš„ä¸²è¡Œç‰ˆæœ¬ï¼ˆéœ€è¦ä»åŸæ–‡ä»¶å¯¼å…¥ï¼‰
        print("[evaluate_step_flags] Using serial version (not implemented here)")
        raise NotImplementedError("Serial version not included in parallel implementation")

def apply_step_mask(batch,
                   step_flags: List[List[bool]],
                   good_scale: float = 1.0,
                   bad_scale: float = 0.2,
                   neg_bad_scale: float = -0.2,
                   use_vectorized: bool = True,
                   mask_tensor: torch.Tensor = None):
    """
    å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œå¯é€‰æ‹©ä½¿ç”¨å‘é‡åŒ–æˆ–åŸç‰ˆæœ¬
    
    Args:
        batch: æ‰¹æ¬¡æ•°æ®
        step_flags: stepè¯„ä¼°ç»“æœ
        good_scale, bad_scale, neg_bad_scale: ç¼©æ”¾å› å­
        use_vectorized: æ˜¯å¦ä½¿ç”¨å‘é‡åŒ–ç‰ˆæœ¬
        mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensor
    """
    if use_vectorized:
        stats = apply_step_mask_vectorized(
            batch=batch,
            step_flags=step_flags,
            good_scale=good_scale,
            bad_scale=bad_scale,
            neg_bad_scale=neg_bad_scale,
            mask_tensor=mask_tensor  # ä¼ å…¥å¤–éƒ¨mask
        )
        return stats
    else:
        # ä½¿ç”¨åŸæ¥çš„ç‰ˆæœ¬ï¼ˆéœ€è¦ä»åŸæ–‡ä»¶å¯¼å…¥ï¼‰
        print("[apply_step_mask] Using original version (not implemented here)")
        raise NotImplementedError("Original version not included in vectorized implementation")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ç»Ÿä¸€çš„å¤„ç†å™¨ç±»ï¼ˆæ”¯æŒevaluation_typeå’Œapi_max_retriesï¼‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

class ParallelSemanticProcessor:
    """å¹¶è¡Œè¯­ä¹‰å¤„ç†å™¨ï¼Œç”¨äºç®¡ç†æ•´ä¸ªæµç¨‹ï¼Œæ”¯æŒæœ¬åœ°å’ŒAPIè¯„ä¼°"""
    
    def __init__(self, 
                 max_concurrent: int = 20,
                 batch_size_limit: int = 100,
                 model_name: str = "qwen-max",
                 evaluation_type: Literal["local", "api"] = "api",
                 api_max_retries: int = 200):
        self.max_concurrent = max_concurrent
        self.batch_size_limit = batch_size_limit
        self.model_name = model_name
        self.evaluation_type = evaluation_type
        self.api_max_retries = api_max_retries
        
        # æ ¹æ®è¯„ä¼°ç±»å‹è°ƒæ•´é»˜è®¤å‚æ•°
        if evaluation_type == "local":
            # æœ¬åœ°æ¨ç†å»ºè®®è¾ƒå°çš„å¹¶å‘æ•°å’Œæ‰¹æ¬¡å¤§å°
            if max_concurrent > 8:
                print(f"[ParallelSemanticProcessor] Local evaluation: reducing max_concurrent from {max_concurrent} to 8")
                self.max_concurrent = 8
            if batch_size_limit > 32:
                print(f"[ParallelSemanticProcessor] Local evaluation: reducing batch_size_limit from {batch_size_limit} to 32")
                self.batch_size_limit = 32
        
        print(f"[ParallelSemanticProcessor] Initialized with evaluation_type={evaluation_type}")
        print(f"[ParallelSemanticProcessor] Settings: model={model_name}, concurrent={self.max_concurrent}, batch_limit={self.batch_size_limit}, api_retries={self.api_max_retries}")
        
    async def process_batch(self, tokenizer, batch, 
                          good_scale: float = 1.0,
                          bad_scale: float = 0.2,
                          neg_bad_scale: float = -0.2,
                          mask_tensor: torch.Tensor = None) -> Dict:
        """
        å¤„ç†æ•´ä¸ªbatchçš„è¯­ä¹‰è¯„ä¼°å’Œmaskåº”ç”¨
        å¯¹äºadvantage=0çš„æ ·æœ¬ä¼šè·³è¿‡è¯„ä¼°
        
        Args:
            tokenizer: åˆ†è¯å™¨
            batch: æ‰¹æ¬¡æ•°æ®
            good_scale, bad_scale, neg_bad_scale: ç¼©æ”¾å› å­
            mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensorï¼Œshape (bs, resp_len)
                        å¯ä»¥æ˜¯loss_maskæˆ–response_mask
        
        Returns:
            ç»¼åˆç»Ÿè®¡ä¿¡æ¯
        """
        start_time = time.time()
        
        # 1. å¹¶è¡Œè¯„ä¼°step flags
        eval_method = "vLLM" if self.evaluation_type == "local" else "API"
        print(f"[ParallelSemanticProcessor] Starting step evaluation with {eval_method}...")
        eval_start = time.time()
        
        step_flags, eval_stats = await evaluate_step_flags_parallel(
            tokenizer=tokenizer,
            batch=batch,
            model_name=self.model_name,
            evaluation_type=self.evaluation_type,
            max_concurrent=self.max_concurrent,
            batch_size_limit=self.batch_size_limit,
            mask_tensor=mask_tensor,  # ä¼ å…¥å¤–éƒ¨mask
            api_max_retries=self.api_max_retries  # ä¼ å…¥APIé‡è¯•æ¬¡æ•°
        )
        
        eval_time = time.time() - eval_start
        print(f"[ParallelSemanticProcessor] Step evaluation completed in {eval_time:.2f}s")
        
        # 2. å‘é‡åŒ–åº”ç”¨mask
        print("[ParallelSemanticProcessor] Applying step mask...")
        mask_start = time.time()
        
        mask_stats = apply_step_mask_vectorized(
            batch=batch,
            step_flags=step_flags,
            good_scale=good_scale,
            bad_scale=bad_scale,
            neg_bad_scale=neg_bad_scale,
            mask_tensor=mask_tensor  # ä¼ å…¥å¤–éƒ¨mask
        )
        
        mask_time = time.time() - mask_start
        print(f"[ParallelSemanticProcessor] Step mask applied in {mask_time:.2f}s")
        
        # 3. åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        
        combined_stats = {
            "total_processing_time": total_time,
            "evaluation_time": eval_time,
            "mask_application_time": mask_time,
            "evaluation_stats": eval_stats,
            "mask_stats": mask_stats,
            "speedup_info": {
                "parallel_evaluation": True,
                "vectorized_masking": True,
                "max_concurrent": self.max_concurrent,
                "evaluation_type": self.evaluation_type,
                "using_vllm": self.evaluation_type == "local",
                "model_name": self.model_name,
                "api_max_retries": self.api_max_retries
            }
        }
        
        print(f"[ParallelSemanticProcessor] Total processing time: {total_time:.2f}s")
        return combined_stats
    
    def process_batch_sync(self, tokenizer, batch, mask_tensor: torch.Tensor = None, **kwargs) -> Dict:
        """
        åŒæ­¥ç‰ˆæœ¬çš„batchå¤„ç†
        
        Args:
            tokenizer: åˆ†è¯å™¨
            batch: æ‰¹æ¬¡æ•°æ®
            mask_tensor: å¤–éƒ¨ä¼ å…¥çš„mask tensor
            **kwargs: å…¶ä»–å‚æ•°
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            self.process_batch(tokenizer, batch, mask_tensor=mask_tensor, **kwargs)
        )