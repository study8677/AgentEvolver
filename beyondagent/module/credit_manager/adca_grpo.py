# -*- coding: utf-8 -*-
# PRM step â†’ (optional) group-level standardization on steps â†’ per-trajectory projection (optional) â†’ suffix-sum on steps â†’ broadcast to tokens
from __future__ import annotations
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
import math

# =========================
# Hyper & small utilities
# =========================

@dataclass
class PRMHyper:
    # æƒé‡ï¼šä¸€è‡´æ€§æ­¥çš„æƒé‡å¤§ï¼Œä¸ä¸€è‡´æ€§æ­¥çš„æƒé‡å°ï¼ˆç”¨äº allocationï¼‰
    consistent_scale: float = 1.0
    pos_unconsistent_scale: float = 0.2   # æˆåŠŸè½¨è¿¹é‡Œçš„ BAD æ­¥æƒé‡
    neg_unconsistent_scale: float = 0.2   # å¤±è´¥è½¨è¿¹é‡Œçš„ GOOD æ­¥æƒé‡
    eps: float = 1e-8
    do_batch_norm: bool = True          # æ˜¯å¦åšç»„å†… z-scoreï¼ˆæŒ‰ step çº§ï¼Œallocation/decouple ä¼šç”¨åˆ°ï¼‰
    equal_trajectory_weight: bool = True  # True=æ¯æ¡è½¨è¿¹ç­‰æƒï¼ˆGRPOï¼‰ï¼›False=æŠŠæ‰€æœ‰ step æ‹‰å¹³æˆä¸€ä¸ªå¤§æ ·æœ¬ï¼ˆGSPOï¼‰
    fix_base: float = 0.2                 # fix æ–¹æ¡ˆçš„åŸºç¡€å¹…åº¦ï¼ˆgood=+base, bad=-baseï¼‰
    alpha: float = 1.0                   # PRMæƒé‡å¹³è¡¡ç³»æ•°
    orm_distribution: str = "last_step"   # ORMåˆ†é…æ–¹å¼ï¼š"last_step" æˆ– "all_steps"
    enable_length_normalization: bool = False  # æ˜¯å¦å¯ç”¨é•¿åº¦æ­£åˆ™åŒ–ï¼ˆé™¤ä»¥sqrt(K)ï¼‰

def _ensure_tensor(x, device, dtype=None):
    """ç¡®ä¿è¾“å…¥è½¬æ¢ä¸ºæŒ‡å®šè®¾å¤‡å’Œç±»å‹çš„å¼ é‡"""
    if torch.is_tensor(x):
        t = x.to(device=device)
        if dtype is not None:
            t = t.to(dtype)
        return t
    return torch.as_tensor(x, device=device, dtype=dtype)

def _num_steps_from_step_ids(step_ids_row: torch.Tensor) -> int:
    """æ ¹æ®step_idsè®¡ç®—è½¨è¿¹ä¸­çš„æ­¥æ•°"""
    if step_ids_row.numel() == 0:
        return 0
    m = torch.amax(step_ids_row)
    return int(m.item() + 1) if m.item() >= 0 else 0

def _align_flags(flags: List[bool], K: int, is_success: bool) -> List[bool]:
    """å¯¹é½flagsé•¿åº¦ä¸æ­¥æ•°Kï¼Œä¸è¶³æ—¶ç”¨é»˜è®¤å€¼å¡«å……"""
    if len(flags) == K:
        return list(flags)
    default_flag = True if is_success else False
    if len(flags) < K:
        return list(flags) + [default_flag] * (K - len(flags))
    else:
        return list(flags[:K])

# =========================
# Group normalization helpers (group-wise, step-level)
# =========================

def _group_zscore_on_steps(
    step_rewards_raw: List[List[float]],
    group_ids: torch.Tensor,
    hyper: PRMHyper,
) -> List[List[float]]:
    """å¯¹ step å¥–åŠ±åšâ€œç»„å†…â€å‡å‡å€¼/é™¤æ–¹å·®æ ‡å‡†åŒ–ã€‚
    - equal_trajectory_weight=True: æ¯æ¡è½¨è¿¹ç­‰æƒï¼›ç»„å‡å€¼ = è½¨è¿¹å‡å€¼çš„å‡å€¼ï¼›
      ç»„æ–¹å·® = è½¨è¿¹å†…ç›¸å¯¹ç»„å‡å€¼çš„å‡æ–¹å·®çš„å‡å€¼ï¼ˆsecond-moment around group meanï¼‰
    - equal_trajectory_weight=False: æ‹‰å¹³æœ¬ç»„æ‰€æœ‰ step ä¸€èµ·ç®—
    """
    if not hyper.do_batch_norm:
        return [list(r) for r in step_rewards_raw]

    B = len(step_rewards_raw)
    gids = group_ids.view(-1).tolist()
    g2idx: Dict[int, List[int]] = {}
    for i, g in enumerate(gids):
        g2idx.setdefault(int(g), []).append(i)

    step_rewards_std: List[List[float]] = [[] for _ in range(B)]
    eps = float(hyper.eps)

    for _, idxs in g2idx.items():
        if hyper.equal_trajectory_weight:
            # === è½¨è¿¹ç­‰æƒï¼šå…ˆå‡å€¼çš„å‡å€¼ï¼Œå†å‡æ–¹å·®çš„å‡å€¼ ===
            n_traj = 0
            mu_acc = 0.0
            for i in idxs:
                ri = step_rewards_raw[i]
                if not ri:
                    continue
                n_traj += 1
                # è½¨è¿¹å‡å€¼ç´¯åŠ ï¼ˆç­‰æƒï¼‰
                mu_acc += (math.fsum(ri) / len(ri))
            if n_traj == 0:
                mu_g, sd_g = 0.0, 1.0
            else:
                mu_g = mu_acc / n_traj
                # ç»„æ–¹å·® = è½¨è¿¹å†…å›´ç»• mu_g çš„å‡æ–¹å·®ï¼Œå†å¯¹è½¨è¿¹åšç­‰æƒå¹³å‡
                second_moments_sum = 0.0
                for i in idxs:
                    ri = step_rewards_raw[i]
                    if not ri:
                        continue
                    second_moments_sum += (math.fsum((x - mu_g) * (x - mu_g) for x in ri) / len(ri))
                var_g = (second_moments_sum / n_traj) if n_traj > 0 else 0.0
                sd_g = math.sqrt(var_g + eps)
        else:
            # === æ‹‰å¹³ï¼šä¸¤éæµå¼ç»Ÿè®¡ï¼ˆé¿å… flat åˆ—è¡¨ä¸ tensor è½¬æ¢çš„å·¨å¤§å¼€é”€ï¼‰===
            total_cnt = 0
            total_sum = 0.0
            # pass1: ç»Ÿè®¡å…¨ç»„æ€»æ­¥æ•°ä¸æ€»å’Œ â†’ å‡å€¼
            for i in idxs:
                ri = step_rewards_raw[i]
                if not ri:
                    continue
                total_cnt += len(ri)
                total_sum += math.fsum(ri)

            if total_cnt == 0:
                mu_g, sd_g = 0.0, 1.0
            else:
                mu_g = total_sum / total_cnt
                # pass2: ç´¯åŠ äºŒé˜¶åå·® â†’ population varianceï¼ˆä¸ unbiased=False å¯¹é½ï¼‰
                M2 = 0.0
                for i in idxs:
                    ri = step_rewards_raw[i]
                    if not ri:
                        continue
                    M2 += math.fsum((x - mu_g) * (x - mu_g) for x in ri)
                var = M2 / total_cnt
                sd = math.sqrt(var)
                sd_g = sd if sd >= eps else eps

        inv = 1.0 / (sd_g + 1e-12)
        for i in idxs:
            ri = step_rewards_raw[i]
            if not ri:
                step_rewards_std[i] = []
            else:
                # ä¸åŸé€»è¾‘ä¸€è‡´ï¼šæŒ‰ç»„ç»Ÿè®¡é‡é€æ­¥æ ‡å‡†åŒ–
                step_rewards_std[i] = [float((x - mu_g) * inv) for x in ri]

    return step_rewards_std


def _per_traj_scale_to_target_sum(
    r_std: List[float],
    target_sum: float,
    eps: float,
) -> List[float]:
    """å°†è½¨è¿¹çš„stepå¥–åŠ±æŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼Œä½¿æ€»å’Œç­‰äºç›®æ ‡å€¼
    
    å½“å½“å‰æ€»å’Œæ¥è¿‘0æ—¶ï¼Œå°†ç›®æ ‡å€¼å‡åŒ€åˆ†é…ç»™æ‰€æœ‰step
    
    Args:
        r_std: æ ‡å‡†åŒ–åçš„stepå¥–åŠ±åˆ—è¡¨
        target_sum: ç›®æ ‡æ€»å’Œå€¼
        eps: æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
        
    Returns:
        ç¼©æ”¾åçš„stepå¥–åŠ±åˆ—è¡¨
    """
    if len(r_std) == 0:
        return []
    cur = sum(r_std)
    if abs(cur) <= eps:
        return [target_sum / len(r_std) for _ in r_std]
    scale = target_sum / cur
    return [float(x * scale) for x in r_std]



# =========================
# Builders for 4 schemes
# =========================
def _build_fix(
    orm_scores: torch.Tensor,
    step_flags: List[List[bool]],
    step_ids: torch.Tensor,
    group_ids: torch.Tensor,
    hyper: PRMHyper,
) -> List[List[float]]:
    """æ–¹æ¡ˆ1ï¼šfix â€”â€” å›ºå®šåŸºæ•°å¥–åŠ±æ„é€  + è½¨è¿¹æœ€åstepçš„ORMç¬¦å·è°ƒæ•´
    
    ç®—æ³•åŸç†ï¼š
      1. åŸºç¡€å¥–åŠ±æ„é€ ï¼šæ ¹æ®step flagsæ„é€ å›ºå®šå¹…åº¦çš„step-levelå¥–åŠ±
         - GOODæ­¥éª¤ï¼š+fix_base
         - BADæ­¥éª¤ï¼š-fix_base
      2. è½¨è¿¹æœ€åstepçš„ORMç¬¦å·è°ƒæ•´ï¼šæ ¹æ®ORMåˆ†æ•°ç¬¦å·ï¼Œåœ¨è½¨è¿¹æœ€åä¸€æ­¥æ·»åŠ æ–¹å‘æ§åˆ¶é¡¹
         - æˆåŠŸè½¨è¿¹(ORM>0)ï¼šæœ€åä¸€æ­¥å¥–åŠ± += +1
         - å¤±è´¥è½¨è¿¹(ORMâ‰¤0)ï¼šæœ€åä¸€æ­¥å¥–åŠ± += -1
    
    ä¼˜åŠ¿å‡½æ•°ç‰¹æ€§ï¼š
      - å¥–åŠ±å¹…åº¦å›ºå®šï¼Œä¸éšè½¨è¿¹é•¿åº¦å˜åŒ–
      - é€šè¿‡ORMç¬¦å·è°ƒæ•´ç¡®ä¿å¥–åŠ±æ–¹å‘ä¸ORMä¸€è‡´
      - é€‚ç”¨äºç®€å•çš„äºŒå…ƒå¥–åŠ±åœºæ™¯
    
    Args:
        orm_scores (torch.Tensor): å®Œæ•´ORMåˆ†æ•°ï¼Œshape (B,)ï¼Œç”¨äºç¡®å®šå¥–åŠ±æ–¹å‘
        step_flags (List[List[bool]]): æ¯æ¡è½¨è¿¹çš„stepçº§åˆ«GOOD/BADæ ‡å¿—
        step_ids (torch.Tensor): stepæ ‡è¯†ç¬¦ï¼Œshape (B, L_resp)ï¼Œ-1è¡¨ç¤ºéresponse token
        group_ids (torch.Tensor): ç»„æ ‡è¯†ç¬¦ï¼Œç”¨äºç»„å†…å½’ä¸€åŒ–ï¼Œshape (B,)
        hyper (PRMHyper): PRMè¶…å‚æ•°é…ç½®ï¼Œä¸»è¦ä½¿ç”¨fix_baseå‚æ•°
        
    Returns:
        List[List[float]]: æ¯æ¡è½¨è¿¹çš„step-levelå¥–åŠ±åˆ—è¡¨ï¼Œé•¿åº¦ä¸stepæ•°ä¸€è‡´
        
    Example:
        orm_scores = [2.5, -1.5]  # ç¬¬ä¸€æ¡è½¨è¿¹æˆåŠŸï¼Œç¬¬äºŒæ¡è½¨è¿¹å¤±è´¥
        step_flags = [[True, False, True], [False, True]]  # ä¸¤æ¡è½¨è¿¹çš„stepæ ‡å¿—
        hyper.fix_base = 0.2
        # è¾“å‡ºç¤ºä¾‹ï¼š
        # [[0.2, -0.2, 0.2],  # ç¬¬ä¸€æ¡è½¨è¿¹ï¼š+0.2-0.2+0.2+1.0 = 1.2
        #  [-0.2, 0.2]]       # ç¬¬äºŒæ¡è½¨è¿¹ï¼š-0.2+0.2-1.0 = -1.0
    """
    B = step_ids.size(0)
    prm_rewards_raw: List[List[float]] = []
    base = float(hyper.fix_base)
    
    # ---- 1. æ„é€ åŸå§‹ PRM å¥–åŠ± ----
    for i in range(B):
        # è·å–å½“å‰è½¨è¿¹çš„stepæ•°é‡
        K = _num_steps_from_step_ids(step_ids[i])
        if K == 0:
            prm_rewards_raw.append([]); continue
            
        # å¯¹é½step flagsé•¿åº¦ï¼Œç¡®ä¿ä¸stepæ•°é‡ä¸€è‡´
        flags = _align_flags(step_flags[i] if i < len(step_flags) else [], K, is_success=True)
        
        # æ„é€ åŸºç¡€PRMå¥–åŠ±ï¼šGOODæ­¥éª¤ä¸º+baseï¼ŒBADæ­¥éª¤ä¸º-base
        r = [(+base if f else -base) for f in flags]
        
        # åŸºäºORMåˆ†æ•°ç¬¦å·è°ƒæ•´æœ€åä¸€æ­¥å¥–åŠ±ï¼Œç¡®ä¿æ•´ä½“å¥–åŠ±æ–¹å‘ä¸ORMä¸€è‡´
        orm_sign = 1.0 if float(orm_scores[i].item()) > 0 else -1.0
        if len(r) > 0:
            r[-1] += orm_sign

        prm_rewards_raw.append(r)

    # ---- 2. ç»„å†… z-score (æ ‡å‡†åŒ–) ----
    # ä½¿ç”¨ _group_zscore_on_steps æ¥åšç»„å†…æ ‡å‡†åŒ–
    prm_rewards_norm = _group_zscore_on_steps(prm_rewards_raw, group_ids, hyper)
    return prm_rewards_norm

def _build_allocation(
    orm_scores: torch.Tensor,
    step_flags: List[List[bool]],
    step_ids: torch.Tensor,
    group_ids: torch.Tensor,
    hyper: PRMHyper,
) -> List[List[float]]:
    """
    æ–¹æ¡ˆ2ï¼šallocation â€”â€” ä¸€è‡´æ€§æƒé‡ç“œåˆ† + ç»„å†…å‡å‡å€¼ä¸­å¿ƒåŒ–
    
    ç®—æ³•åŸç†ï¼š
      1. ä¸€è‡´æ€§æƒé‡ç“œåˆ†ï¼šæ ¹æ®ORMç¬¦å·å’Œstep flagsä¸ºæ¯ä¸ªstepåˆ†é…æƒé‡ï¼Œç¡®ä¿è½¨è¿¹å¥–åŠ±å’Œç­‰äºORMç¬¦å·
         - æˆåŠŸè½¨è¿¹ï¼šä¸€è‡´æ€§æ­¥éª¤æƒé‡é«˜ï¼Œä¸ä¸€è‡´æ€§æ­¥éª¤æƒé‡ä½
         - å¤±è´¥è½¨è¿¹ï¼šä¸€è‡´æ€§æ­¥éª¤æƒé‡ä½ï¼Œä¸ä¸€è‡´æ€§æ­¥éª¤æƒé‡é«˜
      2. ç»„å†…å‡å‡å€¼ä¸­å¿ƒåŒ–ï¼šå¯¹æ•´ä¸ªbatchçš„stepå¥–åŠ±è¿›è¡Œç»„å†…ä¸­å¿ƒåŒ–å¤„ç†ï¼Œè·å¾—çœŸæ­£çš„ä¼˜åŠ¿å‡½æ•°
      
    ä¼˜åŠ¿å‡½æ•°ç‰¹æ€§ï¼š
      - ä¿æŒå¥–åŠ±ç¬¦å·ä¸ORMä¸€è‡´
      - é€šè¿‡æƒé‡åˆ†é…ä½“ç°æ­¥éª¤é‡è¦æ€§å·®å¼‚
      - ç»„å†…å‡å‡å€¼å¾—åˆ°ç›¸å¯¹ä¼˜åŠ¿å€¼
      
    Args:
        orm_scores (torch.Tensor): å®Œæ•´ORMåˆ†æ•°ï¼Œshape (B,)ï¼Œç”¨äºç¡®å®šå¥–åŠ±æ–¹å‘å’Œæƒé‡åˆ†é…ç­–ç•¥
        step_flags (List[List[bool]]): æ¯æ¡è½¨è¿¹çš„stepçº§åˆ«GOOD/BADæ ‡å¿—
        step_ids (torch.Tensor): stepæ ‡è¯†ç¬¦ï¼Œshape (B, L_resp)
        group_ids (torch.Tensor): ç»„æ ‡è¯†ç¬¦ï¼Œç”¨äºç»„å†…å½’ä¸€åŒ–ï¼Œshape (B,)
        hyper (PRMHyper): PRMè¶…å‚æ•°é…ç½®
        
    Returns:
        List[List[float]]: æ¯æ¡è½¨è¿¹çš„step-levelä¼˜åŠ¿å¥–åŠ±ï¼Œå·²è¿›è¡Œç»„å†…å‡å‡å€¼å¤„ç†
    """
    B = step_ids.size(0)

    # ---------- å·¥å…· ----------
    def _p95(vals):
        if not vals:
            return 0.0
        s = sorted(vals)
        k = int(round(0.95 * (len(s) - 1)))
        return float(s[k])

    mean_eps = getattr(hyper, "zscore_mean_tol", 0.05)  # ç»„å†…å‡å€¼å®¹å·®
    std_tol  = getattr(hyper, "zscore_std_tol", 0.2)    # std å…è®¸åç¦» 1 çš„å¹…åº¦ => åŒºé—´ [1-std_tol, 1+std_tol]
    small_mag_threshold = getattr(hyper, "small_mag_threshold", 0.05)

    # ---- ç¬¬ä¸€é˜¶æ®µï¼šç”ŸæˆåŸå§‹PRMå¥–åŠ±ï¼ˆä¸€è‡´æ€§æƒé‡ç“œåˆ†ï¼Œé€è½¨è¿¹å¥–åŠ±å’Œ = ORMç¬¦å·ï¼‰----
    step_rewards_raw: List[List[float]] = []

    # ç›‘æ§ï¼šæƒé‡å æ¯” / é€€åŒ–è®¡æ•° / å‰ç½®ä¸€è‡´æ€§ä¸å˜é‡
    unit_weights: List[float] = []
    pos_consistent_shares: List[float] = []
    neg_consistent_shares: List[float] = []
    degenerate_total_w_count = 0
    pre_norm_sign_agree_flags: List[float] = []

    # å¤šæ•°æ´¾ä¸€è‡´æ€§ï¼ˆåŸºäº PRM æ ‡æ³¨ï¼‰
    pos_major_good = pos_cnt = 0
    neg_major_bad  = neg_cnt = 0

    # è®°å½• flags ä¾›åç»­ r_norm è®¡ç®— GAP
    flags_cache: List[List[bool]] = []

    for i in range(B):
        # è·å–å½“å‰è½¨è¿¹çš„stepæ•°é‡
        K = _num_steps_from_step_ids(step_ids[i])
        if K == 0:
            step_rewards_raw.append([]); flags_cache.append([]); continue

        # æ ¹æ®ORMåˆ†æ•°ç¬¦å·ç¡®å®šè½¨è¿¹ç±»å‹å’Œæƒé‡åˆ†é…ç­–ç•¥
        raw_orm = float(orm_scores[i].item())
        is_success = bool(raw_orm > 0)

        # å¯¹é½ flags
        flags_i = _align_flags(step_flags[i] if i < len(step_flags) else [], K, is_success)
        flags_cache.append(flags_i)

        # GOOD/BAD æ•°
        n_g = sum(1 for f in flags_i if f)
        n_b = K - n_g

        # ä¸€è‡´/ä¸ä¸€è‡´æƒé‡
        if is_success:
            # æˆåŠŸè½¨è¿¹ï¼šä¸€è‡´æ€§æ­¥éª¤(GOOD)æƒé‡é«˜ï¼Œä¸ä¸€è‡´æ€§æ­¥éª¤(BAD)æƒé‡ä½
            w_g, w_b = hyper.consistent_scale, hyper.pos_unconsistent_scale
            sgn = +1.0
        else:
            # å¤±è´¥è½¨è¿¹ï¼šä¸€è‡´æ€§æ­¥éª¤(BAD)æƒé‡ä½ï¼Œä¸ä¸€è‡´æ€§æ­¥éª¤(GOOD)æƒé‡é«˜
            w_g, w_b = hyper.neg_unconsistent_scale, hyper.consistent_scale
            sgn = -1.0
            
        # æƒé‡å½’ä¸€åŒ–ï¼šç¡®ä¿è½¨è¿¹æ€»å¥–åŠ±ç­‰äºORMç¬¦å·
        total_w = n_g * w_g + n_b * w_b
        if total_w <= hyper.eps:
            unit = 0.0
            degenerate_total_w_count += 1
        else:
            unit = 1.0 / total_w
        unit_weights.append(unit)

        # è½¨è¿¹ raw å¥–åŠ±ï¼ˆsum == sgn æˆ–é€€åŒ–ä¸º 0ï¼‰
        r_raw = [sgn * (w_g * unit) if f else sgn * (w_b * unit) for f in flags_i]
        step_rewards_raw.append([float(x) for x in r_raw])

        # ç›‘æ§ï¼šä¸€è‡´æ€§æƒé‡å æ¯”ï¼ˆpos: GOOD ä¸€è‡´ï¼›neg: BAD ä¸€è‡´ï¼‰
        if total_w > hyper.eps:
            if is_success:
                pos_consistent_shares.append((n_g * w_g) / total_w)
            else:
                neg_consistent_shares.append((n_b * w_b) / total_w)

        # ç›‘æ§ï¼špre-norm ä¸å˜é‡ï¼ˆsum(r_raw) ä¸ ORM ç¬¦å·åº”ä¸€è‡´ï¼‰
        raw_sum = sum(r_raw)
        raw_orm_sign = 1.0 if raw_orm > 0 else -1.0
        pre_norm_sign_agree_flags.append(1.0 if (raw_sum * raw_orm_sign) > 0 else 0.0)

        # å¤šæ•°æ´¾ä¸€è‡´æ€§ï¼ˆPRM æ ‡æ³¨ vs ORM æ–¹å‘ï¼‰
        if raw_orm > 0:
            pos_cnt += 1
            if n_g > n_b:
                pos_major_good += 1
        else:
            neg_cnt += 1
            if n_b >= n_g:
                neg_major_bad += 1

    # ---- ç¬¬äºŒé˜¶æ®µï¼šç»„å†… z-score æ ‡å‡†åŒ–ï¼ˆè·å¾—çœŸæ­£çš„ä¼˜åŠ¿å‡½æ•°ï¼‰----
    # ä½¿ç”¨ _group_zscore_on_steps å‡½æ•°è¿›è¡Œæ ‡å‡†åŒ–
    r_norm = _group_zscore_on_steps(step_rewards_raw, group_ids, hyper)

    # ç›‘æ§ï¼šç»„å†…å‡å€¼/æ–¹å·®ï¼ˆæŒ‰ group èšåˆæ‰€æœ‰ stepï¼‰
    gid_list = group_ids.view(-1).tolist()
    group_vals: Dict[int, List[float]] = {}
    all_abs_rnorm: List[float] = []
    for i in range(B):
        g = int(gid_list[i])
        vals = r_norm[i]
        if not vals:
            continue
        group_vals.setdefault(g, []).extend(vals)
        all_abs_rnorm.extend(abs(x) for x in vals)

    group_mean_abs = []
    group_std = []
    zscore_bad_group_cnt = 0
    for g, vals in group_vals.items():
        t = torch.tensor(vals, dtype=torch.float32)
        m = float(t.mean().item())
        s = float(t.std(unbiased=False).item())
        group_mean_abs.append(abs(m))
        group_std.append(s)
        if (abs(m) > mean_eps) or (s < (1 - std_tol)) or (s > (1 + std_tol)):
            zscore_bad_group_cnt += 1

    r_norm_group_mean_abs_p95 = _p95(group_mean_abs) if group_mean_abs else 0.0
    r_norm_group_std_p95 = _p95(group_std) if group_std else 0.0

    # ç›‘æ§ï¼šGOOD/BAD çš„ r_norm å¯åˆ†æ€§ï¼ˆæŒ‰ ORM æ­£è´Ÿåˆ†åˆ«åº¦é‡ï¼‰
    gap_pos_list = []
    gap_neg_list = []
    for i in range(B):
        vals = r_norm[i]
        if not vals:
            continue
        flags_i = flags_cache[i]
        raw_orm = float(orm_scores[i].item())
        good_vals = [v for v, f in zip(vals, flags_i) if f]
        bad_vals  = [v for v, f in zip(vals, flags_i) if not f]
        if raw_orm > 0:
            if good_vals and bad_vals:
                gap_pos_list.append(float(torch.tensor(good_vals).mean() - torch.tensor(bad_vals).mean()))
        else:
            if good_vals and bad_vals:
                gap_neg_list.append(float(torch.tensor(bad_vals).mean() - torch.tensor(good_vals).mean()))
    good_bad_rnorm_gap_pos = float(torch.tensor(gap_pos_list).mean().item()) if gap_pos_list else 0.0
    good_bad_rnorm_gap_neg = float(torch.tensor(gap_neg_list).mean().item()) if gap_neg_list else 0.0

    # ç›‘æ§ï¼šå°å¹…åº¦æ¯”ä¾‹ï¼ˆæ˜¯å¦è¢«ç¨€é‡Šï¼‰
    if all_abs_rnorm:
        rnorm_small_mag_ratio = float(sum(1 for x in all_abs_rnorm if x < small_mag_threshold) / len(all_abs_rnorm))
    else:
        rnorm_small_mag_ratio = 0.0

    # ---------- ç¬¬ä¸‰é˜¶æ®µï¼šç»„å†…æ ‡å‡†åŒ– ORM å¹¶å åŠ åˆ° r_normï¼ˆä¸ decouple ä¸€è‡´çš„åˆ†é…ç­–ç•¥ï¼‰ ----------
    alpha = getattr(hyper, "alpha", 1.0)
    orm_distribution = getattr(hyper, "orm_distribution", "last_step")

    orm_list = orm_scores.detach().cpu().tolist()
    g2idx: Dict[int, List[int]] = {}
    for i, g in enumerate(gid_list):
        g2idx.setdefault(int(g), []).append(i)

    orm_scores_std = [0.0] * B
    for _, idxs in g2idx.items():
        group_vals_orm = [orm_list[i] for i in idxs]
        t = torch.tensor(group_vals_orm, dtype=torch.float32)
        m = t.mean()
        s = t.std(unbiased=False)
        if s <= hyper.eps:
            for i in idxs:
                orm_scores_std[i] = float(orm_list[i] - m.item())
        else:
            denom = s.item() + 1e-12
            for i in idxs:
                orm_scores_std[i] = float((orm_list[i] - m.item()) / denom)

    combined_rewards: List[List[float]] = []
    # ç›‘æ§ï¼šORM/PRM ä¸»å¯¼åº¦ & åç½®ä¸€è‡´æ€§
    per_traj_attr_abs_sum = []
    per_traj_out_abs_sum  = []
    per_traj_out_last_abs = []
    sum_step_reward_sign_agree_flags: List[float] = []

    for i in range(B):
        steps_i = r_norm[i]
        if not steps_i:
            combined_rewards.append([]); continue
        K = len(steps_i)
        ostd = orm_scores_std[i]

        # ç»„åˆ
        if orm_distribution == "last_step":
            arr = [alpha * x for x in steps_i]
            arr[-1] = arr[-1] + ostd
        elif orm_distribution == "all_steps":
            arr = [alpha * x + ostd for x in steps_i]
        else:
            raise ValueError(f"Unknown orm_distribution: {orm_distribution}")

        combined_rewards.append([float(v) for v in arr])

        # ç›‘æ§ï¼šä¸»å¯¼åº¦ï¼ˆä¸ decouple å¯¹é½ï¼‰
        a_abs = sum(abs(alpha * x) for x in steps_i)          # Î± * Î£|r_norm|
        if orm_distribution == "last_step":
            o_abs = abs(ostd)                                 # Î£|ORM|ï¼ˆlast_step æ¨¡å¼ï¼‰
            o_last = abs(ostd)
        else:
            o_abs = K * abs(ostd)                             # all_stepsï¼šæ¯æ­¥éƒ½æœ‰åŒä¸€ orm_std
            o_last = abs(ostd)

        per_traj_attr_abs_sum.append(float(a_abs))
        per_traj_out_abs_sum.append(float(o_abs))
        per_traj_out_last_abs.append(float(o_last))

        # åç½®ä¸€è‡´æ€§ï¼šâˆ‘(combined_step_reward) vs åŸå§‹ ORM ç¬¦å·
        raw_orm_sign = 1.0 if float(orm_scores[i].item()) > 0.0 else -1.0
        if sum(arr) * raw_orm_sign > 0:
            sum_step_reward_sign_agree_flags.append(1.0)
        else:
            sum_step_reward_sign_agree_flags.append(0.0)

    # outcome_share_last_mean & alpha_effective
    shares = []
    for a_abs, o_last in zip(per_traj_attr_abs_sum, per_traj_out_last_abs):
        denom = o_last + a_abs + 1e-12
        shares.append(float(o_last / denom))
    outcome_share_last_mean = float(sum(shares) / max(1, len(shares)))

    alpha_ratios = []
    for a_abs, o_abs in zip(per_traj_attr_abs_sum, per_traj_out_abs_sum):
        denom = o_abs + 1e-12
        alpha_ratios.append(float(a_abs / denom))
    alpha_effective = float(sum(alpha_ratios) / max(1, len(alpha_ratios)))

    sum_step_reward_sign_agree = float(sum(sum_step_reward_sign_agree_flags) / max(1, len(sum_step_reward_sign_agree_flags)))

    # post-norm ä¸å˜é‡ï¼ˆz-score åæŒ‰ç† sumâ‰ˆ0ï¼‰
    post_norm_sum_vals = []
    for vals in r_norm:
        if vals:
            post_norm_sum_vals.append(sum(vals))
    post_norm_sum_mean = float(torch.tensor(post_norm_sum_vals, dtype=torch.float32).mean().item()) if post_norm_sum_vals else 0.0

    # å¤šæ•°æ´¾ä¸€è‡´æ€§ï¼ˆä¸ decouple æŒ‡æ ‡å¯¹é½ï¼Œä¾¿äºæ¨ªå‘æ¯”è¾ƒï¼‰
    pos_rate = float(pos_major_good / max(1, pos_cnt))
    neg_rate = float(neg_major_bad  / max(1, neg_cnt))

    # ---------- æ±‡æ€»æŒ‡æ ‡ ----------
    alloc_stats = {
        # Â§1 æƒé‡åˆ†é…æ˜¯å¦æŒ‰è®¾è®¡å·¥ä½œ
        "prm_allocation/consistent_weight_share_pos": float(torch.tensor(pos_consistent_shares).mean().item()) if pos_consistent_shares else 0.0,
        "prm_allocation/consistent_weight_share_neg": float(torch.tensor(neg_consistent_shares).mean().item()) if neg_consistent_shares else 0.0,
        "prm_allocation/unit_weight_mean": float(torch.tensor(unit_weights).mean().item()) if unit_weights else 0.0,
        "prm_allocation/unit_weight_p95": _p95(unit_weights),
        "prm_allocation/degenerate_total_w_count": float(degenerate_total_w_count),

        # Â§2 z-score æœ‰æ•ˆæ€§
        "prm_allocation/r_norm_group_mean_abs_p95": r_norm_group_mean_abs_p95,
        "prm_allocation/r_norm_group_std_p95": r_norm_group_std_p95,
        "prm_allocation/zscore_bad_group_cnt": float(zscore_bad_group_cnt),

        # Â§3 PRM æ ‡æ³¨ä¸ r_norm çš„å…³ç³»
        "prm_allocation/good_bad_rnorm_gap_pos": good_bad_rnorm_gap_pos,
        "prm_allocation/good_bad_rnorm_gap_neg": good_bad_rnorm_gap_neg,
        "prm_allocation/rnorm_small_mag_ratio": rnorm_small_mag_ratio,

        # Â§4 ä¸å˜é‡æ£€æŸ¥
        "prm_allocation/pre_norm_sum_sign_agree": float(sum(pre_norm_sign_agree_flags) / max(1, len(pre_norm_sign_agree_flags))),
        "prm_allocation/post_norm_sum_mean": post_norm_sum_mean,

        # Â§6 ï¼ˆå åŠ  ORM åçš„ï¼‰ä¸»å¯¼åº¦ä¸ä¸€è‡´æ€§
        "prm_allocation/outcome_share_last_mean": outcome_share_last_mean,
        "prm_allocation/alpha_effective": alpha_effective,
        "prm_allocation/sum_step_reward_sign_agree": sum_step_reward_sign_agree,

        # å¤šæ•°æ´¾ä¸€è‡´æ€§ï¼ˆå’Œ decouple å¯¹é½ï¼Œä¾¿äºæ¨ªå‘æ¯”è¾ƒï¼‰
        "prm_allocation/pos_traj_prm_good_majority_rate": pos_rate,
        "prm_allocation/neg_traj_prm_bad_majority_rate": neg_rate,
    }

    return combined_rewards, alloc_stats


import math
from typing import List, Dict
import torch

def _build_decouple(
    orm_full_scores: torch.Tensor,
    step_flags: List[List[bool]],
    step_ids: torch.Tensor,
    group_ids: torch.Tensor,
    hyper: "PRMHyper"
) -> List[List[float]]:
    """
    æ–¹æ¡ˆ4ï¼šdecouple â€”â€” PRM å’Œ ORM åˆ†åˆ«æ ‡å‡†åŒ–åç»„åˆ
    
    Args:
        enable_length_normalization: æ˜¯å¦å¯ç”¨é•¿åº¦æ­£åˆ™åŒ–ï¼ˆé™¤ä»¥sqrt(K)ï¼‰
                                   - True: å¯¹æ¯æ¡è½¨è¿¹çš„å¥–åŠ±é™¤ä»¥sqrt(è½¨è¿¹é•¿åº¦)ï¼ŒæŠ‘åˆ¶é•¿è½¨è¿¹ä¼˜åŠ¿
                                   - False: ä¸è¿›è¡Œé•¿åº¦æ­£åˆ™åŒ–ï¼Œä¿æŒåŸå§‹ç»„åˆå¥–åŠ±
    
    æ ¸å¿ƒåŒºåˆ«ï¼š
    1. ä¸è¿›è¡Œsqrt: combined_reward ç›´æ¥ä½¿ç”¨
    2. è¿›è¡Œsqrt: combined_reward * (1/sqrt(K))ï¼Œå…¶ä¸­Kæ˜¯è½¨è¿¹é•¿åº¦
    
    å½±å“åˆ†æï¼š
    - å¯ç”¨sqrtä¼šé™ä½é•¿è½¨è¿¹çš„æ•´ä½“å¥–åŠ±å¹…åº¦ï¼Œä½¿ä¸åŒé•¿åº¦è½¨è¿¹æ›´å…¬å¹³
    - ä¸å¯ç”¨sqrtæ—¶ï¼Œé•¿è½¨è¿¹å¯èƒ½å› ä¸ºç´¯ç§¯æ›´å¤šå¥–åŠ±è€Œè¢«è¿‡åº¦åå¥½
    """
    
    B = step_ids.size(0)
    alpha = hyper.alpha
    orm_distribution = hyper.orm_distribution
    enable_length_normalization = hyper.enable_length_normalization # æ–°å¢å‚æ•°æ§åˆ¶æ˜¯å¦è¿›è¡Œsqrté•¿åº¦æ­£åˆ™åŒ–

    # ---- 1. æ„é€ åŸºç¡€ PRM å¥–åŠ± ----
    prm_rewards_raw: List[List[float]] = []
    for i in range(B):
        K = _num_steps_from_step_ids(step_ids[i])
        if K == 0:
            prm_rewards_raw.append([])
            continue
        flags = _align_flags(step_flags[i] if i < len(step_flags) else [], K, is_success=True)
        prm_rewards = [hyper.fix_base if f else -hyper.fix_base for f in flags]
        prm_rewards_raw.append(prm_rewards)

    # ---- 2. å¯¹ PRM å¥–åŠ±åšç»„å†… z-score æ ‡å‡†åŒ– ----
    prm_rewards_std = _group_zscore_on_steps(prm_rewards_raw, group_ids, hyper)
    
    # ---- 3. å¯¹ ORM åˆ†æ•°åšç»„å†…æ ‡å‡†åŒ– ----
    orm_scores = orm_full_scores.cpu().tolist()
    gids = group_ids.view(-1).tolist()
    g2idx: Dict[int, List[int]] = {}
    for i, g in enumerate(gids):
        g2idx.setdefault(int(g), []).append(i)
    
    orm_scores_std = [0.0] * B
    for _, idxs in g2idx.items():
        group_orms = [orm_scores[i] for i in idxs]
        if len(group_orms) == 0:
            continue
        orm_tensor = torch.tensor(group_orms, dtype=torch.float32)
        orm_mean = orm_tensor.mean()
        orm_std = orm_tensor.std(unbiased=False)
        if orm_std <= hyper.eps:
            for i in idxs:
                orm_scores_std[i] = float(orm_scores[i] - orm_mean.item())
        else:
            for i in idxs:
                orm_scores_std[i] = float((orm_scores[i] - orm_mean.item()) / (orm_std.item() + 1e-12))
    
    # ---- 4. ç»„åˆæ ‡å‡†åŒ–çš„ PRM å’Œ ORM ----
    combined_rewards: List[List[float]] = []
    
    # ä¸ºç»Ÿè®¡å‡†å¤‡å®¹å™¨
    per_traj_attr_abs_sum = []   # Î± * |PRM_std| çš„é€è½¨è¿¹æ€»å’Œï¼ˆä¸å« ORMï¼‰
    per_traj_out_abs_sum  = []   # ORM_std çš„é€è½¨è¿¹æ€»å’Œï¼ˆall_steps: K * |orm_std|ï¼›last_step: |orm_std|ï¼‰
    per_traj_out_last_abs = []   # æœ€åä¸€æ­¥ä¸Š ORM çš„ç»å¯¹å€¼ï¼ˆç”¨äº outcome_share_last_meanï¼‰
    sum_sign_agree_flags  = []   # âˆ‘(combined_step_reward) ä¸ åŸå§‹ ORM ç¬¦å·æ˜¯å¦ä¸€è‡´
    pos_major_good, pos_cnt = 0, 0
    neg_major_bad , neg_cnt = 0, 0

    # ä¸º PRM/ORM çš„åˆ†å¸ƒç»Ÿè®¡å‡†å¤‡å®¹å™¨
    flat_attr_vals = []          # æ‰€æœ‰ step çš„ PRM æ ‡å‡†åŒ–å€¼ï¼ˆæœªä¹˜ Î±ï¼‰
    out_vals       = []          # æ¯æ¡è½¨è¿¹ä¸€ä¸ª ORM æ ‡å‡†åŒ–å€¼
    
    for i in range(B):
        if not prm_rewards_std[i]:
            combined_rewards.append([])
            continue

        prm_std = prm_rewards_std[i]
        orm_std = orm_scores_std[i]
        K = len(prm_std)
        # --- PRM/ORM åˆ†å¸ƒç»Ÿè®¡é‡‡æ · ---
        flat_attr_vals.extend(prm_std)
        out_vals.append(float(orm_std))

        # ğŸ”¥ å…³é”®åŒºåˆ«ï¼šæ˜¯å¦è®¡ç®—é•¿åº¦æ­£åˆ™åŒ–å› å­
        if enable_length_normalization:
            length_scale = 1.0 / math.sqrt(max(K, 1))
            print(f"è½¨è¿¹ {i}: é•¿åº¦={K}, é•¿åº¦ç¼©æ”¾å› å­=1/sqrt({K})={length_scale:.4f}")
        else:
            length_scale = 1.0
            print(f"è½¨è¿¹ {i}: é•¿åº¦={K}, æ— é•¿åº¦æ­£åˆ™åŒ– (ç¼©æ”¾å› å­=1.0)")
        
        combined = []
        # é€æ­¥æ„é€  combined_step_rewardï¼Œå¹¶è®¡ç®— per-traj çš„å„ç§å’Œ
        attr_abs_sum = 0.0  # Î± * Î£_j |prm_std[j]|
        for j, prm_reward in enumerate(prm_std):
            if orm_distribution == "last_step":
                if j == K - 1:
                    combined_reward = alpha * prm_reward + orm_std
                else:
                    combined_reward = alpha * prm_reward
            elif orm_distribution == "all_steps":
                combined_reward = alpha * prm_reward + orm_std
            else:
                raise ValueError(f"Unknown orm_distribution: {orm_distribution}")

            final_reward = combined_reward * length_scale
            combined.append(float(final_reward))
            attr_abs_sum += abs(alpha * prm_reward)

        # ORM çš„ç»å¯¹è´¡çŒ®ï¼ˆé€è½¨è¿¹ï¼‰
        if orm_distribution == "last_step":
            out_abs_sum = abs(orm_std)               # åªåœ¨æœ€åä¸€æ­¥åŠ 
            out_last_abs = abs(orm_std)
        else:  # "all_steps"
            out_abs_sum = K * abs(orm_std)           # æ¯æ­¥éƒ½åŠ åŒä¸€ä¸ª orm_std
            out_last_abs = abs(orm_std)

        per_traj_attr_abs_sum.append(float(attr_abs_sum))
        per_traj_out_abs_sum.append(float(out_abs_sum))
        per_traj_out_last_abs.append(float(out_last_abs))

        # âˆ‘(combined_step_reward) ä¸ã€ŒåŸå§‹ã€ORM ç¬¦å·ä¸€è‡´æ€§ï¼ˆä¸ä½¿ç”¨ z-score åçš„ç¬¦å·ï¼‰
        combined_sum = sum(combined)
        raw_orm_sign = 1.0 if float(orm_full_scores[i].item()) > 0.0 else -1.0
        sum_sign_agree_flags.append(1.0 if (combined_sum * raw_orm_sign) > 0 else 0.0)

        # PRM æ ‡æ³¨åœ¨æ­£/è´Ÿè½¨è¿¹ä¸­çš„â€œå¤šæ•°æ´¾â€ä¸€è‡´æ€§
        flags_i = _align_flags(step_flags[i] if i < len(step_flags) else [], K, is_success=True)
        n_g = sum(1 for f in flags_i if f)
        n_b = K - n_g
        if raw_orm_sign > 0:
            pos_cnt += 1
            if n_g > n_b:
                pos_major_good += 1
        else:
            neg_cnt += 1
            if n_b >= n_g:
                neg_major_bad += 1

        combined_rewards.append(combined)

    # === Decouple ç»Ÿè®¡æŒ‡æ ‡ ===
    # 1) PRM/ORM æ ‡å‡†åŒ–ååˆ†å¸ƒçš„ mean/std
    if len(flat_attr_vals) == 0:
        attr_mean, attr_std = 0.0, 0.0
    else:
        t_attr = torch.tensor(flat_attr_vals, dtype=torch.float32)
        attr_mean = float(t_attr.mean().item())
        attr_std  = float(t_attr.std(unbiased=False).item())

    if len(out_vals) == 0:
        out_mean, out_std = 0.0, 0.0
    else:
        t_out = torch.tensor(out_vals, dtype=torch.float32)
        out_mean = float(t_out.mean().item())
        out_std  = float(t_out.std(unbiased=False).item())

    # 2) outcome_share_last_meanï¼š|ORM(æœ€åä¸€æ­¥)| / (|ORM(æœ€åä¸€æ­¥)| + Î± * Î£|PRM_std|)
    shares = []
    for a_abs, o_last in zip(per_traj_attr_abs_sum, per_traj_out_last_abs):
        denom = o_last + a_abs + 1e-12
        shares.append(float(o_last / denom))
    outcome_share_last_mean = float(sum(shares) / max(1, len(shares)))

    # 3) alpha_effectiveï¼šÎ± * Î£|PRM_std| / (Î£|ORM|)ï¼ŒæŒ‰è½¨è¿¹æ±‚æ¯”å†åšå‡å€¼
    alpha_ratios = []
    for a_abs, o_abs, i in zip(per_traj_attr_abs_sum, per_traj_out_abs_sum, range(len(per_traj_out_abs_sum))):
        denom = o_abs + 1e-12
        alpha_ratios.append(float(a_abs / denom))
    alpha_effective = float(sum(alpha_ratios) / max(1, len(alpha_ratios)))

    # 4) âˆ‘(combined_step_reward) ä¸ åŸå§‹ ORM ç¬¦å·ä¸€è‡´çš„æ¯”ä¾‹
    sum_step_reward_sign_agree = float(sum(sum_sign_agree_flags) / max(1, len(sum_sign_agree_flags)))

    # 5) PRM æ ‡æ³¨ä¸ ORM çš„â€œå…¨å±€ä¸€è‡´æ€§â€ï¼ˆå¤šæ•°æ´¾ï¼‰
    pos_rate = float(pos_major_good / max(1, pos_cnt))
    neg_rate = float(neg_major_bad  / max(1, neg_cnt))

    decouple_stats = {
        "prm/decouple/attr_mean": attr_mean,
        "prm/decouple/attr_std": attr_std,
        "prm/decouple/out_mean": out_mean,
        "prm/decouple/out_std": out_std,
        "prm/decouple/outcome_share_last_mean": outcome_share_last_mean,
        "prm/decouple/alpha_effective": alpha_effective,
        "prm/decouple/sum_step_reward_sign_agree": sum_step_reward_sign_agree,
        "prm/decouple/pos_traj_prm_good_majority_rate": pos_rate,
        "prm/decouple/neg_traj_prm_bad_majority_rate": neg_rate,
    }

    # æ³¨æ„ï¼šè¿”å› (rewards, stats) äºŒå…ƒç»„ï¼ˆä»… decouple å¦‚æ­¤ï¼‰ï¼Œå…¶ä½™æ–¹æ¡ˆä»ç„¶åªè¿”å› rewards
    return combined_rewards, decouple_stats
# =========================
# Step â†’ Token broadcast + suffix-sum
# =========================

def suffix_sum_on_steps(step_rewards: List[List[float]]) -> List[List[float]]:
    """è®¡ç®—æ¯ä¸ªè½¨è¿¹stepå¥–åŠ±çš„åç¼€å’Œï¼ˆä»åå¾€å‰ç´¯åŠ ï¼‰
    
    ä¾‹å¦‚: [1, 2, 3] => [6, 5, 3]
    
    Args:
        step_rewards: æ¯æ¡è½¨è¿¹çš„stepå¥–åŠ±åˆ—è¡¨
        
    Returns:
        æ¯æ¡è½¨è¿¹çš„stepä¼˜åŠ¿å€¼åˆ—è¡¨ï¼ˆåç¼€å’Œå½¢å¼ï¼‰
    """
    adv: List[List[float]] = []
    for r in step_rewards:
        if not r:
            adv.append([]); continue
        t = torch.tensor(r, dtype=torch.float32)
        s = torch.flip(torch.cumsum(torch.flip(t, dims=[0]), dim=0), dims=[0])
        adv.append([float(x) for x in s])
    return adv

def broadcast_step_adv_to_tokens(
    step_adv: List[List[float]],
    step_ids: torch.Tensor,
) -> torch.Tensor:
    """å°†stepçº§åˆ«çš„ä¼˜åŠ¿å€¼å¹¿æ’­åˆ°tokençº§åˆ«
    
    æ ¹æ®step_idså°†æ¯ä¸ªstepçš„ä¼˜åŠ¿å€¼èµ‹ç»™å¯¹åº”çš„tokenä½ç½®
    step_idsä¸º-1çš„ä½ç½®ï¼ˆéå“åº”tokenï¼‰ä¿æŒä¸º0
    
    Args:
        step_adv: æ¯æ¡è½¨è¿¹çš„stepä¼˜åŠ¿å€¼åˆ—è¡¨
        step_ids: stepæ ‡è¯†ç¬¦å¼ é‡ï¼Œshape (B, L_resp)ï¼Œ-1è¡¨ç¤ºéå“åº”token
        
    Returns:
        å¹¿æ’­åˆ°tokençº§åˆ«çš„ä¼˜åŠ¿å€¼å¼ é‡ï¼Œshape (B, L_resp)
    """
    device = step_ids.device
    B, L = step_ids.shape
    out = torch.zeros((B, L), device=device, dtype=torch.float32)
    for i in range(B):
        if not step_adv[i]:
            continue
        adv_i = torch.tensor(step_adv[i], device=device, dtype=torch.float32)
        sid_row = step_ids[i]
        valid = sid_row >= 0
        if torch.any(valid):
            sids = sid_row[valid]
            out[i, valid] = adv_i[sids]
    return out

# =========================
# Entry
# =========================

def compute_prm_grpo_advantages(
    batch,                          # DataProto æˆ–å…¼å®¹ç»“æ„ï¼šbatch.batch[...] å¯ç´¢å¼•
    step_flags: List[List[bool]],   # æ¯æ¡è½¨è¿¹çš„ GOOD/BAD æ ‡å¿—
    hyper: Optional[PRMHyper] = None,
    scheme: str = "decouple",   #  "allocation" | "decouple"
) -> dict:
    """
    PRM-GRPOä¼˜åŠ¿å‡½æ•°è®¡ç®—ç»Ÿä¸€å…¥å£
    
    ç®—æ³•æµç¨‹:
      1. æ•°æ®å‡†å¤‡é˜¶æ®µ:
         - æå–å¿…è¦å­—æ®µï¼šstep_ids, group_ids, token_level_rewards
         - è®¡ç®—ORMåˆ†æ•°ï¼šå¯¹token-levelå¥–åŠ±æ±‚å’Œå¾—åˆ°è½¨è¿¹çº§ORMåˆ†æ•°
      2. æ–¹æ¡ˆé€‰æ‹©é˜¶æ®µ:
         - æ ¹æ®schemeå‚æ•°é€‰æ‹©å…·ä½“çš„å¥–åŠ±æ„é€ æ–¹æ¡ˆ
         - è°ƒç”¨å¯¹åº”æ–¹æ¡ˆçš„builderå‡½æ•°æ„é€ step-levelå¥–åŠ±
      3. ä¼˜åŠ¿å€¼è®¡ç®—é˜¶æ®µ:
         - å¯¹step-levelå¥–åŠ±è¿›è¡Œåç¼€å’Œè®¡ç®—å¾—åˆ°step-levelä¼˜åŠ¿å€¼
         - å°†step-levelä¼˜åŠ¿å€¼å¹¿æ’­åˆ°token-level
      4. ç»“æœè¿”å›é˜¶æ®µ:
         - è¿”å›token-levelä¼˜åŠ¿å€¼å’ŒåŸå§‹ORMåˆ†æ•°
    
    ä¼˜åŠ¿å‡½æ•°ç‰¹æ€§:
      - æ”¯æŒå¤šç§å¥–åŠ±æ„é€ æ–¹æ¡ˆï¼Œé€‚åº”ä¸åŒåœºæ™¯éœ€æ±‚
      - ç»Ÿä¸€çš„å¤„ç†æµç¨‹ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
      - å®Œæ•´çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
      - çµæ´»çš„å‚æ•°é…ç½®ï¼Œæ”¯æŒè‡ªå®šä¹‰è¶…å‚æ•°
    
    Args:
        batch: æ•°æ®æ‰¹æ¬¡ï¼ŒåŒ…å«responses, step_ids, group_idsç­‰å­—æ®µ
            - responses: å“åº”å¼ é‡
            - step_ids: stepæ ‡è¯†ç¬¦ï¼Œshape (B, L_resp)ï¼Œ-1è¡¨ç¤ºéresponse token
            - group_ids: ç»„æ ‡è¯†ç¬¦ï¼Œç”¨äºåˆ†ç»„å¤„ç†ï¼Œshape (B,)
            - token_level_rewards: tokençº§å¥–åŠ±ï¼Œç”¨äºè®¡ç®—ORMåˆ†æ•°
        step_flags: æ¯æ¡è½¨è¿¹çš„stepçº§åˆ«GOOD/BADæ ‡å¿—
        hyper: PRMè¶…å‚æ•°é…ç½®ï¼Œè‹¥ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        scheme: å¥–åŠ±æ„é€ æ–¹æ¡ˆ
            - "allocation": ä¸€è‡´æ€§æƒé‡ç“œåˆ† + ç»„å†…å‡å‡å€¼ä¸­å¿ƒåŒ–
            - "decouple": PRMå’ŒORMåˆ†åˆ«æ ‡å‡†åŒ–åç»„åˆ
    
    Returns:
        dict: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸
            - advantages: (B, L_resp) token-levelä¼˜åŠ¿å€¼
            - orm_scalar: (B,) é€æ¡è½¨è¿¹çš„ Â±1
    """
    if hyper is None:
        hyper = PRMHyper()

    # ---- 1. æ•°æ®å‡†å¤‡é˜¶æ®µï¼šæå–å¿…è¦å­—æ®µ ----
    # è·å–è®¾å¤‡ä¿¡æ¯ï¼Œç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
    responses = batch.batch["responses"]
    device = responses.device if torch.is_tensor(responses) else torch.as_tensor(responses).device

    # æå–step_idså’Œgroup_idsï¼Œå¹¶ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    step_ids = _ensure_tensor(batch.batch["step_ids"], device=device, dtype=torch.long)      # (B, L_resp) with -1 for non-response
    # >>> add begin: å¯¹é½åˆ°çœŸå®å“åº”é•¿åº¦ <<<
    target_L = responses.size(1)
    if step_ids.size(1) != target_L:
        if step_ids.size(1) > target_L:
            step_ids = step_ids[:, :target_L]
        else:
            pad = torch.full(
                (step_ids.size(0), target_L - step_ids.size(1)),
                -1, device=step_ids.device, dtype=step_ids.dtype
            )
            step_ids = torch.cat([step_ids, pad], dim=1)
    # <<< add end
    group_ids = _ensure_tensor(batch.batch["group_ids"], device=device, dtype=torch.long).view(-1)

    # ---- 2. æå–token-levelå¥–åŠ± ----
    # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µåè·å–token-levelå¥–åŠ±
    token_keys_try = ["token_level_rewards", "response_token_level_rewards", "token_rewards"]
    token_level_rewards = None
    for k in token_keys_try:
        if k in batch.batch:
            token_level_rewards = _ensure_tensor(batch.batch[k], device=device, dtype=torch.float32)
            break
    if token_level_rewards is None:
        raise KeyError("token-level rewards not found in batch (tried keys: token_level_rewards / response_token_level_rewards / token_rewards)")

    # ---- 3. ORMå¤„ç†ï¼šè®¡ç®—ORMåˆ†æ•° ----
    # å¯¹token-levelå¥–åŠ±æ±‚å’Œå¾—åˆ°è½¨è¿¹çº§ORMåˆ†æ•°ï¼Œç”¨äºå„ä¸ªæ–¹æ¡ˆçš„å¥–åŠ±æ„é€ 
    orm_sum = token_level_rewards.sum(dim=1)   # (B,)
    orm_scores = torch.where(orm_sum > 0, torch.ones_like(orm_sum), -torch.ones_like(orm_sum)).to(dtype=torch.float32)

    # ---- 4. æ–¹æ¡ˆé€‰æ‹©é˜¶æ®µï¼šæ ¹æ®schemeé€‰æ‹©å…·ä½“çš„å¥–åŠ±æ„é€ æ–¹æ¡ˆ ----
    extra_metrics = {}
    scheme = (scheme or "decouple").lower()

    if scheme == "allocation":
        # æ–¹æ¡ˆ2ï¼šallocation â€”â€” ä¸€è‡´æ€§æƒé‡ç“œåˆ† + ç»„å†…å‡å‡å€¼ä¸­å¿ƒåŒ–
        step_rewards, extra_metrics = _build_allocation(orm_scores, step_flags, step_ids, group_ids, hyper)
    elif scheme == "decouple":
        # æ–¹æ¡ˆ4ï¼šdecouple â€”â€” PRMå’ŒORMåˆ†åˆ«æ ‡å‡†åŒ–åç»„åˆ
        step_rewards, extra_metrics = _build_decouple(orm_scores, step_flags, step_ids, group_ids, hyper,)
    else:
        raise ValueError(f"Unknown PRM scheme: {scheme} (expected one of: allocation | decouple)")

    # ---- 5. ä¼˜åŠ¿å€¼è®¡ç®—é˜¶æ®µï¼šstepåç¼€å’Œ + å¹¿æ’­åˆ°token ----
    # å¯¹step-levelå¥–åŠ±è¿›è¡Œåç¼€å’Œè®¡ç®—å¾—åˆ°step-levelä¼˜åŠ¿å€¼
    step_adv = suffix_sum_on_steps(step_rewards)
    # å°†step-levelä¼˜åŠ¿å€¼å¹¿æ’­åˆ°token-level
    advantages = broadcast_step_adv_to_tokens(step_adv, step_ids)

    # ---- 6. ç»“æœè¿”å›é˜¶æ®µï¼šæ„é€ è¿”å›å­—å…¸ ----
    # è¿”å›token-levelä¼˜åŠ¿å€¼å’ŒåŸå§‹ORMåˆ†æ•°
    return {
        "advantages": advantages,        # (B, L_resp) token-levelä¼˜åŠ¿å€¼
        "orm_scores": orm_scores,         # (B,) é€æ¡è½¨è¿¹çš„ Â±1
        "metrics":  extra_metrics,      # âœ… ä»… decouple ä¼šæœ‰
    }
