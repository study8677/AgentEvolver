hydra:
  searchpath:
    - file://external/verl/verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  max_prompt_length: 1024
  max_response_length: 1024
  train_batch_size: 256
  val_batch_size: 256
  return_raw_chat: True

actor_rollout_ref:
  hybrid_engine: True
  actor:
    loss_agg_mode: seq-mean-token-mean
    entropy_mask:
      mode: null
      raw: null
    # ç§»é™¤æ—§çš„ advantage é…ç½®ï¼Œç»Ÿä¸€åˆ° semantic_advantage
  rollout:
    name: vllm
    mode: async
    multi_turn:
      completion_callback: beyondagent.module.trainer.simple_completion_callback.SimpleCompletionCallback
      enable: True
      format: llama3_json
      max_steps: 30
      tool_config_path: ""
    custom_dataflow_cls:
      path: ""
      name: ""
    use_qwen3: True
    max_env_worker: 64
    max_env_len: 4096
    enable_request_id: False

thread_pool:
  max_workers: 5

env_service:
  env_type: "appworld"
  env_url: "http://127.0.0.1:8000"

experience_maker:
  base_url: "http://127.0.0.1:8001"
  workspace_id: "w1_agentflow"
  enable_summarizer: False
  enable_context_generator: False
  retrieve_top_k: 3

save_dir: ./save_dir/save_entropy

# ç»Ÿä¸€çš„è¯­ä¹‰ä¼˜åŠ¿è¯„ä¼°é…ç½® - æ‰€æœ‰è¯­ä¹‰è¯„ä¼°ç›¸å…³å‚æ•°éƒ½åœ¨è¿™é‡Œ
semantic_advantage:
  enable: false                    # æ€»å¼€å…³
  evaluation_type: "local"           # ğŸ”¥ æ–°å¢ï¼šè¯„ä¼°ç±»å‹ ("local" æˆ– "api")
  mask_type: "loss_mask"          # maskç±»å‹
  mode: "semantic"                 # æ¨¡å¼
  good_scale: 1.0                  # å¥½æ­¥éª¤ç¼©æ”¾å› å­
  bad_scale: 0.2                   # åæ­¥éª¤ç¼©æ”¾å› å­
  neg_bad_scale: -0.2              # è´Ÿé¢åæ­¥éª¤ç¼©æ”¾å› å­
  concurrent: 20                   # å¹¶å‘å¤„ç†æ•°é‡
  model: "qwen-max"              # è¯­ä¹‰è¯„ä¼°ä½¿ç”¨çš„æ¨¡å‹
  api_max_retries: 200             # APIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤200æ¬¡