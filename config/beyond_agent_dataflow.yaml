hydra:
  searchpath:
    - file://external/verl/verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  max_prompt_length: 1024
  max_response_length: 1024
  train_batch_size: 256
  val_batch_size: 256
  return_raw_chat: True
  train_files: /mnt/data_aisys_cpfs/zouanni.zan/data/appworld_parquet/train.parquet
  val_files: /mnt/data_aisys_cpfs/zouanni.zan/data/appworld_parquet/dev.parquet

actor_rollout_ref:
  hybrid_engine: True
  actor:
    loss_agg_mode: token-mean
    off_cliprange_high: 1.0   # off-policyæ ·æœ¬çš„cliph
  rollout:
    name: vllm
    mode: async
    debug_llm_io: False
    multi_turn:
      completion_callback: beyondagent.module.trainer.simple_completion_callback.SimpleCompletionCallback
      enable: True
      format: llama3_json
      max_steps: 30
      tool_config_path: ""
    custom_dataflow_cls:
      path: ""
      name: ""
    use_qwen3: True
    context_template: "linear"
    context_template_train_sp_action: False
    max_env_worker: 64
    max_model_len: 20480
    max_env_len: 4096
    enable_request_id: False
    sparse: True
    magnify_success: False
    train_history_infer_token: True

thread_pool:
  max_workers: 5

env_service:
  env_type: "appworld"
  env_url: "http://127.0.0.1:8000"
  env_feedin_preference: code # code, text, box

task_manager:
  train_data_path: tasks_explored.train.json # tasks will be explored once if set. use it if you want to keep the same explorations.
  val_data_path: tasks_explored.val.json # tasks will be explored once if set. use it if you want to keep the same explorations.
  llm_client: qwen-plus # policy, or other llm name provided in DashScopeClient
  # 0 - stop exploration
  n: 0
  bs: ${data.train_batch_size} # this should be the same as train_batch_size
  num_explore_threads: ${thread_pool.max_workers}

  mixture:
    use_original_tasks: True
    synthetic_data_ratio: 0.0
    shuffle: True

  strategy: random
  strategy_args:
    max_explore_step: 15
    max_llm_retries: 3
    env_url: ${env_service.env_url}
    exploration_llm_temperature: 1.0
    exploration_llm_top_p: 1.0
    exploration_llm_top_k: 1
    task_summary_history_length: 10 # the size of sliding windows used in task summarization


experience_maker:
  base_url: "http://127.0.0.1:8001"
  workspace_id: "default"
  enable_summarizer: False
  enable_context_generator: False
  retrieve_top_k: 3
  updated_freq: 0   # æ›´æ–°ç»éªŒæ± çš„é¢‘ç‡(è¿™é‡Œè¡¨ç¤ºkä¸ªsteps)
  val_summarizer_save: False


hybrid_experience_training:
  enable: False
  val_rollout_expmode: "mixed"    # devé›†ä¸Šrolloutæ˜¯å¦åŠ ç»éªŒ: ["mixed", "all", "woexp"]
  train_rollout_expmode: "mixed"  # trainé›†ä¸Šrolloutæ˜¯å¦åŠ ç»éªŒ: ["mixed", "all", "woexp"]
  rollout_expratio: 0.5           # rolloutæ—¶åœ¨groupå†…éƒ¨åŠ ç»éªŒçš„æ¯”ä¾‹, train&val ä¿æŒä¸€è‡´
  train_sample_expmode: "keep"    # trainé›†ä¸Šrolloutä¹‹åè½¬æ¢æˆè®­ç»ƒæ ·æœ¬ä¿ç•™/å‰”é™¤/æ··åˆç»éªŒ: ["keep", "discard", "hybrid"]
  train_sample_keepratio: 0.5     # task-levelå¤šå°‘æ¯”ä¾‹é€‰æ‹©ä¿ç•™ç»éªŒ
  experience_template: "\n\nSome Related Experience to help you to complete the task:<EXP>{}</EXP>"  # æ’å…¥ç»éªŒçš„æ¨¡ç‰ˆ

trainer:
  val_only: False

# ç»Ÿä¸€çš„è¯­ä¹‰ä¼˜åŠ¿è¯„ä¼°é…ç½® - æ‰€æœ‰è¯­ä¹‰è¯„ä¼°ç›¸å…³å‚æ•°éƒ½åœ¨è¿™é‡Œ
semantic_advantage:
  enable: false                    # æ€»å¼€å…³
  evaluation_type: "local"           # ğŸ”¥ æ–°å¢ï¼šè¯„ä¼°ç±»å‹ ("local" æˆ– "api")
  mask_type: "loss_mask"          # maskç±»å‹
  mode: "semantic"                 # æ¨¡å¼
  consistent_scale: 1.0                  # å¥½æ­¥éª¤ç¼©æ”¾å› å­
  pos_unconsistent_scale: 0.2                   # åæ­¥éª¤ç¼©æ”¾å› å­
  neg_unconsistent_scale: -0.2              # è´Ÿé¢åæ­¥éª¤ç¼©æ”¾å› å­
  concurrent: 20                   # å¹¶å‘å¤„ç†æ•°é‡
  model: "qwen-max"              # è¯­ä¹‰è¯„ä¼°ä½¿ç”¨çš„æ¨¡å‹
  api_max_retries: 200             # APIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤200æ¬¡
  adv_norm:
    enable: true         # æ˜¯å¦åšadvå½’ä¸€åŒ–
    level: "batch"       # "batch" æˆ– "group"
    group_size: null     # groupæ¨¡å¼ä¸‹å¯è¦†ç›–é»˜è®¤repeatå¤§å°
    norm_style: "nostd"
  llm_evaluation_log_dir: null  # è®¾ç½®ä¸ºå…·ä½“è·¯å¾„æ¥å¯ç”¨ï¼Œä¾‹å¦‚ï¼š"/path/to/llm_evaluation_logs"
  prm_grpo:
    enable_prm_grpo: false
    prm_scheme: allocation_c           # fix | allocation | allocation_c | decouple
    do_batch_norm: true
    equal_trajectory_weight: true
    fix_base: 0.2
    alpha: 1.0
    orm_distribution: last_step
